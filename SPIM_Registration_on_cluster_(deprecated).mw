{{Infobox Plugin
| name                   = Selective Plane Illumination Microscopy Registration on a cluster
| software               = Fiji
| author                 = Stephan Saalfeld, Pavel Tomancak
| maintainer             = Pavel Tomancak
| filename               = n.a.
| released               = May 2013
| latest version         = May 2013
| category               = [[:Category:Registration|SPIM Registration]]
| website                = [http://www.mpi-cbg.de/research/research-groups/pavel-tomancak.html Pavel Tomancak's homepage]
}}

Light sheet microscopy such as SPIM produces enormous amounts of data especially when used in long-term time-lapse mode. In order to view and in some cases analyze the data it is necessary to process them which involves registration of the views within time-points, correction of sample drift across the time-lapse registration, fusion of data into single 3d image per time-point which may require multiview deconvolution and 3d rendering of the fused volumes. Here we describe how to perform such processing in parallel on a cluster computer.

We will use data derived from the [http://microscopy.zeiss.com/microscopy/en_de/products/imaging-systems/lightsheet-z-1.html Lightsheet.Z1] a commercial realisation of SPIM offered by Zeiss. The Lightsheet.Z1 data can be truly massive and cluster computing may well be the only way to deal with the data deluge coming of the microscope.

Every cluster is different both in terms of the used hardware and the software running on it, particularly the scheduling system. Here we use cluster computer at the MPI-CBG that consists of '''44''' nodes each with '''12''' Intel Xeon E5-2640 processor running @ 2.50 GHz and '''128GB''' of memory. The cluster nodes have access to 200TB of data storage provided by a dedicated Lustre Server architecture. For more info on Lustre see [http://en.wikipedia.org/wiki/Lustre_(file_system) here], suffice to say that it is optimised for high performance input/output (read/write) operations which is crucial for the SPIM data volumes.

Each node of this cluster runs CentOS 6.3 Linux distribution. The queuing system running on the MPI-CBG cluster is '''LSF''' - [http://en.wikipedia.org/wiki/Platform_LSF Load Sharing Facility]. The basic principles of job submission are the same across queuing systems, but the exact syntax will of course differ.

== Pre-requisites ==

=== Saving data on Lighsheet.Z1 ===

The Lightsheet.Z1 data are saved into the proprietary Zeiss file format '''.czi'''. Zeiss is working with [http://loci.wisc.edu/software/bio-formats Bioformats] to make the .czi files compatible with Open Source platforms including Fiji. At the moment Fiji can only open .czi files that are saved as a single file per view where the left and right illumination images have been fused into one image inside the Zeiss ZEN software. This situation is going to change, for now if you want to process the data with Fiji, save them in that way (TBD).

=== Transferring data ===

First we have to get the data to the cluster. This is easier said then done because we are potentially talking about terabytes of data. Moving data over 10Gb Ethernet is highly recommended otherwise the data transfer will take days. 

Please note that currently the Zeiss processing computer does not support data transfer while the acquisition computer is acquiring which means that you need to include the transfer time when booking the instruments. Transferring 5TB of data over shared 1Gb network connection will take a while...

=== Installing Fiji on the cluster ===

Change to a directory where you have sufficient privileges to install software.

 cd /sw/users/tomancak/packages

Download Fiji nightly build from [http://fiji.sc/Downloads Fiji's download page]. In all likelihood you will need the Linux (64 bit) version (unless you are of course using some sort of Windows/Mac cluster). Unzip and unpack the tarball

 gunzip fiji-linux64.tar.gz
 tar -xvf fiji-linux64.tar

Change to the newly created Fiji-app directory and [http://fiji.sc/Update_Fiji#Command-line_usage update] Fiji from the command line  

  ./ImageJ-linux64 --update update

''Note: The output that follows may have some warnings and errors, but as long as it says somewhere Done: Checksummer and Done: Downloading... everything should be fine.''

Done, you are ready to use Fiji on the cluster.
 
== Saving data as tif ==

As a first step we will open the .czi files and save them as '''.tif'''. This is necessary because Fiji's bead based registration currently cannot open the .czi files. Opening hundreds of files several GB each sequentially and re-saving them as tif may take a long time on a single computer. We will use the cluster to speed-up that operation significantly.

''Note: The Lustre filesystem on MPI-CBG cluster is made to be able to handle such situation, where hundreds of nodes are going to simultaneously read and write big files to it. If your cluster is using a Network File System (NFS) this may not be such a good idea...''

We have an 240 time-point, 3 view dataset (angles 325, 235 and 280) in a directory 

 cd /projects/tomancak_lightsheet/Tassos/
 ls -1
 spim_TL1_Angle235.czi
 spim_TL1_Angle280.czi
 spim_TL1_Angle325.czi
 spim_TL1_Angle235.czi
 spim_TL1_Angle280.czi
 spim_TL1_Angle325.czi
 ...

we create a subdirectory '''jobs/resaving''' and change to it

 mkdir jobs/resaving
 cd jobs/resaving

Now we create a bash script '''create-resaving-jobs''' that will generate the so called job files that will be submitted to the cluster nodes (I use nano but any editor will do)

<source lang="bash">
#!/bin/bash
dir="/projects/tomancak_lightsheet/Tassos"
jobs="$dir/jobs/resaving"

mkdir -p $jobs

for i in `seq 1 240`
do
  	job="$jobs/resave-$i.job"
        echo $job
        echo "#!/bin/bash" > "$job"
        echo "xvfb-run -a /sw/users/tomancak/packages/Fiji.app/ImageJ-linux64 -Ddir=$dir
-Dtimepoint=$i -Dangle=280 -- --no-splash ${jobs}/resaving.bsh" >> "$job"
        chmod a+x "$job"
done
</source>

We customize the script by editing the parameters inside it. One can think of it as a template that is used as a starting point to adapt to the particular situation. For instance we can change the directory '''dir''' where the data are to be found, the place where the output will go '''jobs''', the number of time-points to process '''for i in `seq 1 240`''' and most importantly the angle to be processed '''-Dangle=280'''. The strategy we follow here is to create jobs to process one angle at a time for all available time-points.

This will generate 240 '''resave-<number>.job''' files in the current directory

 /projects/tomancak_lightsheet/Tassos/jobs/resaving/resave-1.job
 /projects/tomancak_lightsheet/Tassos/jobs/resaving/resave-2.job
 /projects/tomancak_lightsheet/Tassos/jobs/resaving/resave-3.job
 ...
 /projects/tomancak_lightsheet/Tassos/jobs/resaving/resave-240.job

each one of those files looks like this

<source lang="bash">
#!/bin/bash
xvfb-run -a /sw/users/tomancak/packages/Fiji.app/ImageJ-linux64
-Ddir=/projects/tomancak_lightsheet/Tassos
-Dtimepoint=38 -Dangle=280 -- --no-splash
 /projects/tomancak_lightsheet/Tassos/jobs/resaving/resaving.bsh
</source>

running this job a any cluster node will launch fiji in a so-called virtual frame buffer (the nodes don't have graphics capabilities but we can simulate that) and then inside Fiji it will launch a '''Beanshell script''' called '''resaving.bsh''' passing it thee parameters : the directory (/projects/tomancak_lightsheet/Tassos), the time-point (38) and the angle (280).

Lets create that script in the current directory

<source lang="java">
import ij.IJ;
import ij.ImagePlus;
import java.lang.Runtime;
import java.io.File;
import java.io.FilenameFilter;

runtime = Runtime.getRuntime();

dir = System.getProperty( "dir" );
timepoint = System.getProperty( "timepoint" ); 
angle = System.getProperty( "angle" );

IJ.run("Bio-Formats Importer", "open=" + dir + "spim_TL" + timepoint + "_Angle" + angle + ".czi" +
" autoscale color_mode=Default specify_range view=[Standard ImageJ] stack_order=Default              
t_begin=1000 t_end=1000 t_step=1");
IJ.saveAs("Tiff ", dir + "spim_TL" + timepoint + "_Angle" + angle + ".tif");

/* shutdown */
runtime.exit(0);
</source>

''Note: The t_begin=1000 t_end=1000 are parameters passed to Bioformats Opener. This is a hack. The .czi files think that they are part of a long time-lapse despite the fact that they were saved as single, per angle .czi. In order to trick bioformats into opening just the timepoint which contains actual data we set the time coordinate way beyond the actual length of the time-course (in this case 240). This results in Bioformats importing the "last" timepoint in the series which contains the data. This will change!

Now we need to create yet another bash script (last one) called  '''submit jobs'''

<source lang="bash">
#!/bin/bash

for file in `ls ${1} | grep ".job$"`
do
  	bsub -q short -n 1 -R span[hosts=1] -o "out.%J" -e "err.%J" ${1}/$file
done
</source>

This will look into the current directory for all files ending with '''.job''' (we created them before) and submit all of them to the cluster with the '''bsub''' command.

 bsub -q short -n 1 -R span[hosts=1] -o "out.123345" -e "err.123456" ./resave-1.job

* ''-q short''  selects the queue to which the job will be submitted (this one allows jobs that run up to 4 hours on MPI-CBG cluster).
* ''-n 1'' specifies how many processors will the job request, in this case just one (we will only open and save one file)
* -R span[hosts=1] says that if we were requesting more than one processor, they would be on a single physical machine (host).
* ''-o "out.%j"'' will create output file called out.<job_number> in the current directory
* ''-e "err.%J"'' will send errors to the file called err.<job_number> in the current directory
* ''${1}/$file'' will evaluate to ./resave-<number>.job i.e. the bash script that the cluster node will run - see above

Lets recapitulate. We have created '''create-resaving-jobs''' that, when executed, creates many '''resave-<number>.job''' files. Those are going to be submitted to the cluter using '''submit-jobs''' and on the cluster nodes will run '''resaving.bsh''' using Fiji and the specified parameters.

So lets run it. We need to issue the following command 

 ./submit-jobs .

the dot at the end tells submit job where to look for .job files i.e. in the current directory. What you should see is something like this

 [tomancak@madmax resaving]$ ./submit-jobs .
 Job <445490> is submitted to queue <short>.
 Job <445491> is submitted to queue <short>.
 Job <445492> is submitted to queue <short>.
 ....

We can monitor running jobs with 

 bjobs -r
 JOBID   USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME 
 445490  tomanca RUN   short      madmax      n17         *ave-1.job May  1 11:36 
 445491  tomanca RUN   short      madmax      n33         *ave-2.job May  1 11:36
 445492  tomanca RUN   short      madmax      n01         *ave-3.job May  1 11:36
 445493  tomanca RUN   short      madmax      n18         *ave-4.job May  1 11:36
 445494  tomanca RUN   short      madmax      n21         *ave-5.job May  1 11:36

or whatever your submission system offers. At the end of the run we will have a lot of '''err.<job_number>''' and '''out.<job_number>''' files in the working directory.

 err.445490
 out.445490
 err.445491
 out.445491
 ....

The err.* are hopefully empty. The out.* contain Fiji log output if any. In this case it should look something like this. Most importantly in the directory /projects/tomancak_lightsheet/Tassos we now have for each .czi file a corresponding .tif file which was the goal of the whole exercise

 ls *Angle280*
 spim_TL1_Angle280.czi
 spim_TL1_Angle280.tif
 spim_TL2_Angle280.czi
 spim_TL2_Angle280.tif
 spim_TL3_Angle280.czi
 spim_TL3_Angle280.tif
 ...

We can remove the .czi files (rm *.czi) as we do not need them anymore (but check some of the tifs first!).

Now we must repeat the whole procedure for the other two angles (325 and 235). Open create-resaving-jobs and change 280 to 325 and follow the recipe again. There are of course ways to automate that.

On our cluster powered by the Lustre filesystem the resaving operation takes only minutes. Imagine what is happening - up to 480 processors are accessing the file system reading .czi files and immediately resaving it to that very same filesystem as tif - all at the same time. The files are 1.8GB each. Beware: this may not work at all on lesser filesystems - the Lustre is made for this.

== Registration ==

=== Bead-based multi-view registration ===

The first real step in the SPIMage processing pipeline, after re-saving as .tif, is to register the views within each timepoint. We will use for that the bead based registration plug-in in Fiji. The principle of the plug-in are described [http://fiji.sc/SPIM_Registration_Method here] while the parameters are discussed [http://fiji.sc/SPIM_Bead_Registrationn here].

This description focuses on cluster processing and is less verbose, for details see section on resaving as the principles are the same.

In a directory jobs/registration create bash script '''create-registration-jobs'''

<source lang=bash>
#!/bin/bash
dir="/projects/tomancak_lightsheet/Tassos"
jobs="$dir/jobs/registration_integral_img"

mkdir -p $jobs

for i in `seq 1 240`
do
  	job="$jobs/register-$i.job"
        echo $job
        echo "#!/bin/bash" > "$job"
        echo "xvfb-run -a /sw/users/tomancak/packages/Fiji.app/ImageJ-linux64 -Ddir=$dir 
-Dtimepoint=$i -Dangles=325,280,235 -- --no-splash ${jobs}/registration.bsh$
        chmod a+x "$job"
done
</source>

Run it to create 240 '''registration-<number>.job''' bash scripts

<source lang=bash>
#!/bin/bash
xvfb-run -a /sw/users/tomancak/packages/Fiji.app/ImageJ-linux64 
-Ddir=/projects/tomancak_lightsheet/Tassos -Dtimepoint=603
-Dangles=325,280,235,10,190 -- --no-splash
/projects/tomancak_lightsheet/Tassos/jobs/registration_integral_img/registration.bsh
</source>

which will run '''registration.bsh''' using Fiji

<source lang="java">
import ij.IJ;
import ij.ImagePlus;
import java.lang.Runtime;
import java.io.File;
import java.io.FilenameFilter;

runtime = Runtime.getRuntime();
System.out.println(runtime.availableProcessors() + " cores available for multi-threading");

dir = System.getProperty( "dir" );
timepoint = System.getProperty( "timepoint" );
angles = System.getProperty( "angles" );

IJ.run("Bead-based registration", "select_type_of_registration=Single-channel" + " " +
        "select_type_of_detection=[Difference-of-Mean (Integral image based)] " + " " +
        "spim_data_directory=" + dir + " " +
        "pattern_of_spim=spim_TL{t}_Angle{a}.tif" + " " +
        "timepoints_to_process=" + timepoint + " " +
        "angles_to_process=" + angles + " " +
        "bead_brightness=[Advanced ...]" + " " +
        "subpixel_localization=[3-dimensional quadratic fit (all detections)]" + " " +
        "specify_calibration_manually xy_resolution=1.000 z_resolution=3.934431791305542" + " " +
        "transformation_model=Affine" + " " +
        "channel_0_radius_1=2" + " " +
        "channel_0_radius_2=3" + " " +
        "channel_0_threshold=0.0069"
        );

/* shutdown */
runtime.exit(0);
</source>

on a cluster node when submitted by '''submit-jobs'''

<source lang="bash">
#!/bin/bash

for file in `ls ${1} | grep ".job$"`
do
  	bsub -q short -n 1 -R span[hosts=1] -o "out.%J" -e "err.%J" ${1}/$file
done
</source>

Some tips and tricks

* the bead based registration code is '''NOT''' multi-threaded, thus 1 processor is sufficient (bsub -n 1)
* the registration needs at least as much memory on the node to be able to simultaneously open all views (3x1.8GB here). Since our nodes have 128GB of shared memory it is not really an issue here, we can run registration using 12 cores on one machine at the same time. 
* the crucial parameter for bead based registation is the "channel_0_threshold=0.0069"; determine it on a local workstation using Fiji GUI. Clusters typically do not have graphical interface

=== Time-lapse registration ===

Once the per-time-point registration is finished it is necessary to register all the time-points in the time-series to a reference time-point (to remove potential sample drift during imaging). The parameters for time series registration are described [http://fiji.sc/SPIM_Bead_Registration#How_timelapse_registration_works here].

The time-series registration is not really a cluster type of task as it is run on a single processor in a linear fashion. But since until now we have everything on the cluster filesystem it is useful to execute it here.

It is a very bad idea to execute anything other then submitting jobs on a cluster head node. LSF offers a useful alternative - a special '''interactive''' queue allowing us to connect directly to a free node of the cluster and execute commands interactively.

 bsub -q interactive -Is bash
 Job <445547> is submitted to queue <interactive>.
 <<Waiting for dispatch ...>>
 <<Starting on n27>>

We are now on node 27 and can use the filesystem as if we were on the head node (not every queuing system will enable this).

We create a bash script '''timelapse.interactive'''

<source lang="bash">
#!/bin/bash
xvfb-run -a /sw/users/tomancak/packages/Fiji.app/ImageJ-linux64 
-Ddir=/projects/tomancak_lightsheet/Tassos/ -Dtimepoint=1-240 -Dangles=325,280,235
 -- --no-splash ./time-lapse.bsh
</source>

It calls '''time-lapse.bsh''' that will run fiji with the appropriate parameters for timelapse registration plug-in

<source lang="java">
import ij.IJ;
import java.lang.Runtime;

runtime = Runtime.getRuntime();
System.out.println(runtime.availableProcessors() + " cores available for multi-threading");

dir = System.getProperty( "dir" );
timepoint = System.getProperty( "timepoint" );
angles = System.getProperty( "angles" );

IJ.run("Bead-based registration", "select_type_of_registration=Single-channel" + " " +
        "select_type_of_detection=[Difference-of-Mean (Integral image based)]" + " " +
        "spim_data_directory=" + dir + " " +
        "pattern_of_spim=spim_TL{t}_Angle{a}.tif" + " " +
        "timepoints_to_process=" + timepoint + " " +
        "angles_to_process=" + angles + " " +
        "load_segmented_beads" + " " +
        "subpixel_localization=[3-dimensional quadratic fit (all detections)]" + " " +
        "specify_calibration_manually xy_resolution=1.000 z_resolution=3.934431791305542" + " " +
        "transformation_model=Affine" + " " +
        "channel_0_radius_1=2" + " " +
        "channel_0_radius_2=3" + " " +
        "channel_0_threshold=0.0098" + " " +
        "re-use_per_timepoint_registration" + " " +
        "timelapse_registration" + " " +
        "select_reference=[Manually (specify)]" + " " +
        "reference_timepoint=709"
        );

/* shutdown */
runtime.exit(0);
</source>

Executing the timelapse.interactive 

 ./timelapse.interactive

will start a long stream of timelapse registration output. Its a good idea to redirect it to a file like this:

 ./timelapse.interactive > timelapse_reg.out

We can just as well run the timelapse registration from the head node by issuing

 bsub -q short -n 1 -R span[hosts=1] -o "out.%J" -e "err.%J" ./timelapse.interactive

In this case the output will go into out.<job_number> file in the working directory.

Tips and tricks

* The crucial parameter of timelapse registration is "reference_timepoint=709". It could be either a timepoint with low registration error or a timepoint in the middle of the time series.
* It si important to specify the z_resolution in timelapse.bsh ("specify_calibration_manually xy_resolution=1.000 z_resolution=3.934431791305542" ), otherwise the plugin will open every raw data file to read the metadata which can take quite long.
* the xy_resolution can be set to 1 since the plugin only uses the ratio between xy and z
* For very long time-series where the sample potentially jumps in the field of view it may be necessary to register several segments of the series separately.

== Fusion ==

=== Content based multiview fusion ===

After registration we need to combine the views into a single output image. The content based fusion algorithm in Fiji solves that problem by evaluating local image entropy and weighing differentially the information in areas where several views overlap. For details see [http://fiji.sc/Multi-View_Fusion here].

As before we create a directory jobs/fusion and in there bash script '''create-fusion-jobs'''

<source lang="bash">
#!/bin/bash
dir="/projects/tomancak_lightsheet/Tassos/"
jobs="$dir/jobs"

mkdir -p $jobs

for i in `seq 1 240`
do
  	job="$jobs/fusion/fusion-$i.job"
        echo $job
        echo "#!/bin/bash" > "$job"
        echo "xvfb-run -a /sw/users/tomancak/packages/Fiji.app/ImageJ-linux64 -Xms100g -Xmx100g 
-Ddir=$dir -Dtimepoint=$i -Dangles=325,280,235 -Dreference=120 -Dx=0 -Dy=0 -Dz=0 
-Dw=1936 -Dh=1860 -Dd=1868 -- --no-splash 
/projects/tomancak_lightsheet/Tassos/jobs/fusion/fusion.bsh" >> "$job"
        chmod a+x "$job"
done
</source>

that will generate many '''fusion-<number>.job''' scripts

<source lang="bash">
#!/bin/bash
xvfb-run -a /sw/users/tomancak/packages/Fiji.app/ImageJ-linux64 -Xms100g -Xmx100g 
-Ddir=/projects/tomancak_lightsheet/Tassos/ -Dtimepoint=10 -Dangles=325,280,235 
-Dreference=120 -Dx=0 -Dy=0 -Dz=0 -Dw=1936 -Dh=1860 -Dd=1868 -- --no-splash 
/projects/tomancak_lightsheet/Tassos/jobs/fusion/fusion.bsh
</source>

Each of these will run '''fusion.bsh'''

<source lang="java">
import ij.IJ;
import java.lang.Runtime;

runtime = Runtime.getRuntime();
System.out.println(runtime.availableProcessors() + " cores available for multi-threading");

dir = System.getProperty( "dir" );
timepoint = System.getProperty( "timepoint" );
angles = System.getProperty( "angles" );
reference = System.getProperty( "reference" );
x = System.getProperty( "x" );
y = System.getProperty( "y" );
z = System.getProperty( "z" );
w = System.getProperty( "w" );
h = System.getProperty( "h" );
d = System.getProperty( "d" );
iter = System.getProperty( "iter" );

IJ.run("Multi-view fusion", "select_channel=Single-channel" + " " +
"spim_data_directory=" + dir + " " +
"pattern_of_spim=spim_TL{t}_Angle{a}.tif timepoints_to_process=" + timepoint + " " +
"angles=" + angles + " " +
"fusion_method=[Fuse into a single image]" + " " +
"process_views_in_paralell=All" + " " +
"blending" + " " +
"content_based_weights_(fast," + " " +
"downsample_output=4" + " " +
"registration=[Time-point registration (reference=" + reference + ") of channel 0]" + " " + 
"crop_output_image_offset_x=" + x + " " +
"crop_output_image_offset_y=" + y + " " +
"crop_output_image_offset_z=" + z + " " +
"crop_output_image_size_x=" + w + " " +
"crop_output_image_size_y=" + h + " " +
"crop_output_image_size_z=" + d + " " +
"fused_image_output=[Save 2d-slices, one directory per time-point]");

/* shutdown */
runtime.exit(0);
</source>

on a cluster node when submitted by '''submit-jobs'''

<source lang="bash">
#!/bin/bash

for file in `ls ${1} | grep ".job$"`
do
  	bsub -q short -n 12 -R span[hosts=1] -o "out.%J" -e "err.%J" ${1}/$file
done
</source>

Tips and tricks:

* Fusion is memory intensive no matter what.
* The content based fusion will necessarily degrade image quality. Thus it makes only sense to fuse the image for visualization purposes such as 3D rendering.
* It is not necessary or even possible to 3D render the full resolution data. Thus we use the "downsample_output=4" option to make it 4 times smaller.
* The downsampling also reduces the storage requirements for the fused data which can be unrealistic for full resolution data (tens of terabytes).
* The fusion code is multi-threaded, therefore we request 12 processors on one host "bsub -n 12 -R span[hosts=1]" and request as much memory as possible "fiji-linux64 -Xms100g -Xmx100g". Requesting 12 hosts guarantees all the memory on a single node is available for the job (128GB). It may be difficult to get that when others are running small, single processor jobs on the cluster.
* The integral image mediated weightening is much faster than the traditional gauss method, for large images it may be the only option as one can also run out of 128GB of RAM with this data.

=== Multiview deconvolution ===

Another, more advanced, way to fuse the registered data is multiview deconvolution which will be described eloquently and in details [http://fiji.sc/Multi-View_Deconvolution here]. 

By now you should know the drill... Create a directory jobs/deconvolution and in there a bash script '''create-deconvolution-jobs'''

<source lang="bash">
#!/bin/bash
dir="/projects/tomancak_lightsheet/Tassos"
jobs="$dir/jobs/deconvolution"

mkdir -p $jobs

for i in `seq 1 240`
do
  	job="$jobs/deconvolution-$i.job"
        echo $job
        echo "#!/bin/bash" > "$job"
        echo "xvfb-run -a /sw/users/tomancak/packages/Fiji.app/ImageJ-linux64 -Xms100g -Xmx100g
-Ddir=$dir -Dtimepoint=$i -Dangles=1-6 -Dreference=714 -Dx=36 -Dy=168 -Dz=282 -Dw=1824 
-Dh=834 -Dd=810 -- --no-splash 
/projects/tomancak_lightsheet/Tassos/jobs/deconvolution/deconvolution.bsh" >> "$job"
        chmod a+x "$job"
done
</source>

that will generate many '''deconvolution-<number>.job''' scripts

<source lang="bash">
#!/bin/bash
xvfb-run -a /sw/users/tomancak/packages/Fiji.app/ImageJ-linux64 -Xms100g -Xmx100g 
-Ddir=/projects/tomancak_lightsheet/Valia/Valia -Dtimepoint=1 -Dangles=1-6 -Dreference=714 
-Dx=36 -Dy=168 -Dz=282 -Dw=1824 -Dh=834 -Dd=810 -- --no-splash 
/projects/tomancak_lightsheet/Tassos/jobs/deconvolution/deconvolution.bsh
</source>

Each of these will run '''deconvolution.bsh'''

<source lang="java">
import ij.IJ;
import java.lang.Runtime;

runtime = Runtime.getRuntime();
System.out.println(runtime.availableProcessors() + " cores available for multi-threading");

dir = System.getProperty( "dir" );
timepoint = System.getProperty( "timepoint" );
angles = System.getProperty( "angles" );
reference = System.getProperty( "reference" );
x = System.getProperty( "x" );
y = System.getProperty( "y" );
z = System.getProperty( "z" );
w = System.getProperty( "w" );
h = System.getProperty( "h" );
d = System.getProperty( "d" );
iter = System.getProperty( "iter" );

IJ.run("Multi-view deconvolution", "spim_data_directory=" + dir + " " +
        "pattern_of_spim=spim_TL{t}_Angle{a}.tif" + " " +
        "timepoints_to_process=" + timepoint + " " +
        "angles=" + angles + " " +
        "registration=[Time-point registration (reference=" + reference + ") of channel 0]" + " " +
        "crop_output_image_offset_x=" + x + " " +
        "crop_output_image_offset_y=" + y + " " +
        "crop_output_image_offset_z=" + z + " " +
        "crop_output_image_size_x=" + w + " " +
        "crop_output_image_size_y=" + h + " " +
        "crop_output_image_size_z=" + d + " " +
        "type_of_iteration=[Conditional Probability (fast, precise)]" + " " +
        "number_of_iterations=10" + " " +
        "use_tikhonov_regularization tikhonov_parameter=0.0060" + " " +
        "imglib_container=[Array container]" + " " +
        "compute=[in 512x512x512 blocks]" + " " +
        "compute_on=[CPU (Java)]" + " " +
        "fused_image_output=[Save 2d-slices, one directory per time-point]"
        );

/* shutdown */
runtime.exit(0);
</source>

on a cluster node when submitted by '''submit-jobs'''

<source lang="bash">
#!/bin/bash

for file in `ls ${1} | grep ".job$"`
do
  	bsub -q medium -n 12 -R span[hosts=1] -o "out.%J" -e "err.%J" ${1}/$file
done
</source>

Tips and tricks:

* TBD

== 3D rendering ==

Finally we want generate a beautiful 3d rendering of the downsampled, fused data and run it as movies at conferences... ;-).


We start by creating our old friend, the '''create-render-job''' bash script

<source lang="bash">
#!/bin/bash
dir="/projects/tomancak_lightsheet/Tassos/"
jobs="$dir/jobs"

mkdir -p $jobs

for i in `seq 1 241`
do
  	job="$jobs/3d_rendering/render-$i.job"
        echo $job
        echo "#!/bin/bash" > "$job"
        echo "xvfb-run -as\"-screen 0 1280x1024x24\" 
/sw/users/tomancak/packages/Fiji.app/ImageJ-linux64 -Xms10g -Xmx10g -macro /projects/tomancak_lightsheet/Tassos/jobs/3d_rendering/render.ijm $i" >> "$job"
        chmod a+x "$job"
done
</source>

who will create '''render-<number>.job'''

<source lang="bash">
#!/bin/bash
xvfb-run -as"-screen 0 1280x1024x24" /sw/users/tomancak/packages/Fiji.app/ImageJ-linux64 -Xms10g -Xmx10g -macro /projects/tomancak_lightsheet/Tassos/jobs/3d_rendering/render.ijm 1
</source>

This script is in this case running an ImageJ macro '''render.ijm'''

<source lang="bash">
timepoint = getArgument;

run("Image Sequence...", "open=[/projects/tomancak_lightsheet/Tassos/output/" + timepoint + "/] number=467 starting=1 increment=1 scale=100 file=[.tif] or=[] sort");
rename("output");
setMinAndMax(65, 5300);
run("8-bit");
run("Add...", "value=1 stack");   // because 3d viewer will re-scale the bounding box with 0 in the background
 
run("3D Viewer");
call("ij3d.ImageJ3DViewer.setCoordinateSystem", "false");

call("ij3d.ImageJ3DViewer.add", "output", "None", "output", "0", "true", "true", "true", "1", "0");
call("ij3d.ImageJ3DViewer.select", "output");
wait( 1000 );

call("ij3d.ImageJ3DViewer.setTransform", "1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 -400.0 0.0 0.0 0.0 1.0 ");

call("ij3d.ImageJ3DViewer.setThreshold", "30");
call("ij3d.ImageJ3DViewer.setTransparency", "0.35");

wait( 1000 );
call("ij3d.ImageJ3DViewer.select", "Dummy");
wait( 1000 );
call("ij3d.ImageJ3DViewer.snapshot", "968", "934");
wait( 1000 );
call("ij3d.ImageJ3DViewer.select", "output");
wait( 1000 );
call("ij3d.ImageJ3DViewer.delete");

selectWindow("Snapshot");
saveAs("Tiff", "/projects/tomancak_lightsheet/Tassos/rendering/render_tp" + timepoint + "_Angle0.tif");
call("java.lang.System.exit", 0);
</source>

We submit is as usual using '''./submit-jobs .'''

<source lang="bash">
#!/bin/bash

for file in `ls ${1} | grep ".job$"`
do
  	bsub -q short -n 1 -R span[hosts=1] -o "out.%J" -e "err.%J" ${1}/$file
done
</source>

Tips and tricks

* This approach to making 3d rendering movies is clearly a hack.
* We are using an interplay of Fiji basic functions to pre-process the image by rotating it to the desired position and then calling 3dVIewer to render it at that position while zooming in a bit.
* You can use the TransformJ commands to rotate your fused stack to the orientation you like, possibly crop it if its too big and then open it in 3dViewer.
* You have to master the intricacies of ImageJ macro language to achieve that (like calling windows by name, closing unused windows etc.). Whoever wants to rewrite it into a real script is VERY welcome.
* Even better would be to pass the transformation matrix to the 3dViewer, but it proved unreliable. 
* The key to making it work on a cluster is to provide specific parameters about screen size to the xvfb-run script. Otherwise it doesn't work. Thanks to Stephan Saalfeld for figuring it out.
* The cluster makes it fairly easy to experiment with parameters and angles of view - on a single computer the same task would take days and since we are using ImageJ macro you would not be able to touch the computer.

[[Category:Registration|SPIM Registration]]
