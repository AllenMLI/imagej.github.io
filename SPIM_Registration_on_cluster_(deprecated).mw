Light sheet microscopy such as SPIM produces enormous amounts of data especially when used in long-term time-lapse mode. In order to view and in some cases analyze the data it is necessary to process them which involves registration of the views within time-points, correction of sample drift across the time-lapse registration, fusion of data into single 3d image per time-point which may require multiview deconvolution and 3d rendering of the fused volumes. Here we describe how to perform such processing in parallel on a cluster computer.

We will use data derived from the Lightsheet.Z1 a commercial realisation of SPIM offered by Zeiss. The Lightsheet.Z1 data can be truly massive and cluster computing may well be the only way to deal with the data deluge coming of the microscope.

Every cluster is different both in terms of the used hardware and the software running on it, particularly the scheduling system. Here we use cluster computer at the MPI-CBG that consists of 44 nodes each with 12 Intel Xeon E5-2640 processor running @ 2.50 GHz and 128GB of run. The cluster has access to 200TB of data storage is provided by a dedicated Lustre Server architecture. For more info on Lustre see [http://en.wikipedia.org/wiki/Lustre_(file_system) here], suffice to say that it is optimised for high performance input/output (read/write) operations which is crucial for the SPIM data volumes.

Each node of this cluster runs CentOS 6.3 Linux distribution. The queuing system running on the MPI-CBG cluster is LSF - Load Sharing Facility. The basic principles of job submission are the same across queuing systems, but the exact syntax will of course differ.

== Saving data as tif ==



== Bead-based multi-view registration ==

== Time-lapse registration ==

== Content based multiview fusion ==

== Multiview deconvolution ==

== 3D rendering ==
