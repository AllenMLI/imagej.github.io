== Introduction ==

This tutorial is the starting point for [[TrackMate]] users. It explains how it works by walking you through a simple case, using an easy image. 

The [[TrackMate]] plugin provides a way to semi-automatically segment spots or roughly spherical objects from a 2D or 3D image, and track them over time. It follows the classical scheme, where the segmentation step and the particle-linking steps are separated. Therefore each step is handled in the user interface by a specific panel, and you will go back in forth through them. 
Also, TrackMate has a fishing net with small holes: it will find as much spots as it can, even the ones you are not interested. So there is a step to filter them out before tracking. 
In these views, TrackMate resembles a bit to the Spot Segmentation Wizard of [http://www.bitplane.com/go/products/imaris Imaris™].


== The test image ==

Grab the image we will use for this tutorial [http://fiji.sc/tinevez/TrackMate/FakeTracks.tif here], and open it in Fiji.

[[Image:TrackMate FakeTracks.png|right|border|]]


This is 128x128 stack of 50 frames, uncalibrated. It is noisy, but is still a very easy use case: there is at most 4 spots per frame, they are well separated, they are about the same size and the background is uniform. It is such an ideal case that you would not need TrackMate to deal with it. But for this first tutorial, it will help us getting through TrackMate without being bothered by difficulties.

Also, if you look carefully, you will see that there are two splitting events - where a spot seems to divide in two spots in the next frame, one merging event - the opposite, and a gap closing event - where a spot disappear for one frame then reappear a bit further. TrackMate is made to handle these events, and we will see how. 

<br style="clear: both" />

== Starting TrackMate ==

[[Image:TrackMate MainButtons.png|right|border|]]

With this image selected, launch TrackMate from the menu ''Plugins > Tracking > Track Mate'' or from the [[Using the Command Launcher|Command launcher]]. The Track Mate GUI appears next to the image, displaying the starting dialog panel.

But first, just a few words about its look. The user interface is a single frame divided in a main panel, that displays context-dependent dialogs, and a permanent bottom panel containing  the four main buttons depicted right. 

The '''Next''' button allows to step through the tracking process. It might be disabled depending on the current panel content. For instance, if you do not select a valid image in the first panel, it will be disabled. The '''Previous''' button steps back in the process, without executing actions. For instance, if you go back on the segmentation panel, segmentation will not be re-executed. 

Pressing the '''Load''' button opens a dialog asking you for a TrackMate file (they are plain XML files). Depending on when in the process you saved it, it will load all of its data and put you at the adequate step to pursue or inspect tracking results. The '''Save''' button does of course the converse.

Now is a good time to speak strategy when it comes to saving/restoring. You can save at anytime in TrackMate. If you save just before the tracking process, you will be taken there with the data you generated so far upon loading. TrackMate saves a <u>link to the image file</u> (as an absolute file path) but not the image itself. When loading a TrackMate file, it tries first to retrieve and open this file in ImageJ. So it is a good idea to pre-process, crop, edit metadata and massage the target image first in Fiji, then save it as a .tif, then launch TrackMate. Particularly if you deal with a multi-series file, such as Leica .lif files.

The advantage of this approach is that you can launch TrackMate, immediately press the load button, and everything you need will be loaded and displayed. However, if you need to change with the target file of if it cannot be retrieved, you will have to open the TrackMate XML file and edit its 4th line.

<br style="clear: both" />

== The start panel ==

[[Image:TrackMate StartPanel.png|right|border|]]

This first panel allows you to check the spatial and temporal calibration of your data. It is very important to get it right, since everything afterwards will be based on physical units and not in pixel units (for instance μm and minutes, and not pixels and frames). In our case, that does not matter actually, since our test image has a dummy calibration (1 pixel = 1 pixel). 

What is critical is also to check the dimensionality of the image. In our case, we have a 2D time-lapse of 50 frames. If metadata are not present or cannot be read, ImageJ tends to assume that stack always are Z-stack on a single time-point. 

If the calibration or dimensionality of your data is not right, I recommend changing it in the image metadata itself, using ''Image > Properties'' (Ctrl+Shift+P). The press the 'Refresh' button on the TrackMate start panel to grab changes.

You can also define a sub-region for processing: if you are only interested in finding spots in a defined region of the image, you can use any of the ROI tools of ImageJ to draw a closed area on the image. Once you are happy with it, press the '''Refresh''' button on the panel to pass it to TrackMate. You should see that the '''X''' '''Y''' start and end values change to reflect the bounding box of the ROI you defined. The ROI needs not to be a square. It can be any closed shape. 

If you want to define the min and max '''Z''' and/ or '''T''', you have to edit manually the fields on the panel. 

Defining a smaller area to analyze can be very beneficial to test and inspect for correct parameters, particularly for the segmentation step. In this tutorial, the image is so small and parse that we need not worrying about it.








The segmentation step identifies where the objects are in the time-lapse image. All objects in all frames are identified in this first step (segmentation and tracking are not performed simultaneously). The method used here is the Laplacian of the Gaussian (LoG), followed by identification of regional maxima, or connected components of pixels with intensity strictly higher than all bordering pixels.</p> 

<p>The result is a candidate list of objects identified within the image. Since this approach to segmentation usually results in many false positives, a thresholding step is incorporated to separate true positives from the false positives. The objects can be thresholded on many features (the LoG value at the center of the object, the average brightness of the object within the expected volume, etc), and an automatic thresholding can be performed for any feature based on histogram thresholding. However, the user retains ultimate control and can threshold objects based on all features in any combination.</p>

<p>Following thresholding, the candidate list of objects is refined to a finalized list of objects, which is used as the input for the tracking step.</p>
</li>

<li>
<p><b>Tracking:</b> The tracking algorithm is an implementation of the algorithm described by Jaqaman, K. et al. in the paper 'Robust single-particle tracking in live-cell time-lapse sequences' (Nature Methods, 2008). In short, this algorithm turns tracking into two separate steps, each of which is solved as a Linear Assignment Problem (LAP).</p>

<p>The two steps in this tracking method are: 

<ol>
<li> Links are made between objects in consecutive frames to create track segments (one-to-one)</li>
<li> Links are made between track segments (merge, split, gap closure). </li>
</ol>
</p>

<p>The tracking algorithm allows for the following, biologically defined events:

<ul>
<li>merging</li>
<li>splitting</li>
<li>disappearance</li>
<li>appearance</li>
</ul>
</p>
</li>
</ol>

== Settings ==

<p>The only required setting is the <u>estimated diameter of the objects being tracked</u>, in physical units.</p>

<p>The user can also choose to modify the calibration settings of the image,  and crop the image in any dimension (x, y, z, as well as t).</p>

== Tutorial ==

=== Initialization ===

<p>Before starting the plugin, be sure to have the 2D or 3D image (over time, if tracking) selected. This image should contain roughly "spherical" blobs (for example: nuclei, bacteria, particles) that are to be segmented and tracked. Then, open the TrackMate plugin.</p>

<p>The only required parameter for the plugin is the estimated diameter of the object in physical units. Once this has been entered, adjust calibration settings as needed (if the image is properly calibrated, you do not have to change the calibration settings). You may also crop the image in the x, y, z, and t dimensions. You may want to do this if the image contains objects you do not wish to track, large regions of noise, or if you do not want to track over all frames.</p>

<p>Once all of the parameters have been set, select 'Next.' This initializes TrackMate's segmentation step, which is then executed.</p>

=== Segmentation ===

<p>The segmentation step identifies all potential objects in the image (over all frames). The actual segmentation is performed automatically, and requires no input from the user.</p>

=== Thresholding Segmentation Results ===

<p>You can now threshold the identified objects based on a number of features, the full list of which can be seen in the drop-down menu. The features can be used in combination with one another to refine the results. (Currently, only the logical operator 'and' is supported.)</p>

<p>Select which features you would like to threshold the objects with (LoG is typically a good feature to threshold), and use the histograms to help determine good threshold values. Features can be added or removed using the '+' and '-' buttons, respectively. You may also choose whether to threshold objects above or below the value you pick.</p>

<p>For many features, the 'noisy' objects (false positives) will be clumped together to the right or left end of the histogram in a pseudo-normal distribution. Typically, you will want to choose threshold values which exclude these regions of the histogram (we want only those few objects which meet our criteria).</p>

<p>Note that each feature contains an 'auto' button, which will perform automatic thresholding on that feature (Otsu threshold).</p>

<p>As a concrete example, suppose we want to select all objects based on the LoG value and the mean intensity within their estimated volume. To accomplish this, first select 'LoG Value' from the drop-down, and either press 'auto' or select the value manually using the histogram. Then select the '+' button to add another feature to threshold, and choose 'Mean Intensity' from the drop-down. Again, select the value to threshold on with either the 'auto' button or by choosing the value manually on the histogram.</p>

<p>Once you are finished thresholding, select 'Next' to track the thresholded objects over time.</p>

=== Tracking ===

<p>Once the segmented objects have been thresholded, click 'Next' to track the thresholded objects over time.</p>




[[Category:Tutorials]]
