import os
import markdown_link_extractor
from ij_mw_preprocess_debo import *
import re
from shutil import copyfile

pandoc_failing = ["Versioning",
               "2012-03-20_-_Unit_tests_for_ImageJ_1.46",
               "Deconvolution",
               "Fiji_Contribution_requirements",
               "BigDataServer",
                "2D_Histogram",
                "Alida"]

liquid_syntax_error = ["Batch_Processing", "MATLAB_Scripting", "Sholl_Analysis", "TrakEM2_Scripting",
                    "Hackathon", "Image_Intensity_Processing", "Miji", "Strahler_Analysis",
                    "ImageJ", "Test_Block_Matching_Parameters", "Feature_Extraction", "Developing_ImageJ_in_Eclipse"]



def delete_autogenerated_media(root_out):
    media_dir = images_dir(root_out)
    for filename in os.listdir(media_dir):
        file = os.path.join(media_dir, filename)
        os.remove(file)


def images_dir(root_out):
    return os.path.join(os.path.join(root_out, "images"), "pages")


def catch_media(path_in, root_out):
    content = read_file(path_in)
    pos = -1
    while True:
        match = re.search(r'\[\[File\:([^ |]*)[ ]*\|[ ]*([^ |]*)[ ]*\|[ ]*link=(.*)[ ]*\]\]', content)
        if match:
            if match.start() == pos:
                break
            pos = match.start()
            file_name = match[1]
            print("Copying " + file_name)
            file_out = os.path.join(root_out, file_name)
            file_in = os.path.join(os.path.dirname(path_in), file_name)
            content = content.replace(match[0], "")
            if not os.path.exists(file_in):
                print("Could not find media " + file_in)
                continue
            if os.path.exists(file_out):
                continue
            copyfile(file_in, file_out)
        else:
            break


def delete_autogenerated_pages(root_out):
    pages_dir = os.path.join(root_out, "pages")
    for filename in os.listdir(pages_dir):
        if filename.endswith(".md"):
            file = os.path.join(pages_dir, filename)
            with open(file, 'r') as f:
                lines = f.readlines()
                if lines[1] == autogenerated_line:
                    print("Deleting " + file)
                    os.remove(file)

def iteratively_convert(root_in, root_out, page_title):
    layout = "page"
    metapage = False
    title = page_title
    if "#" in page_title:
        parts = page_title.split("#", 1)
        if parts[0].strip():
            iteratively_convert(root_in, root_out, parts[0])
        return
    if "(" in page_title:
        print("Cannot process names with parentheses: " + page_title)
        return
    if ":" in page_title:
        if "Special:" in page_title:
            # print("Cannot create mediawiki special pages")
            return
        if "Category:" in page_title:
            layout = "category"
            metapage = True
            page_title = page_title.replace(":Category:", "Category:")
            title = page_title.replace("Category:", "")
        else:
            print("Cannot convert pages with colon in title: " + page_title)
            return
    if page_title in pandoc_failing:
        print("Cannot process, pandoc fails: " + page_title)
        return
    if page_title in liquid_syntax_error:
        print("Cannot process, creates invalid syntax: " + page_title)
        return
    path_in = None
    if not metapage:
        path_in = os.path.join(root_in, page_title + ".mw")
        if not os.path.exists(path_in):
            print("Could not find " + page_title)
            return
        title = get_title(path_in)
        catch_media(path_in, images_dir(root_out))
    path_out = os.path.join(os.path.join(root_out, "pages"), page_title + ".md")
    if os.path.exists(path_out):
        return
    convert(path_in, path_out, layout, title)
    convert_links(path_out, root_in, root_out)

def convert_links(path_out, root_in, root_out):
    output = read_file(path_out)
    links = markdown_link_extractor.getlinks(output)
    for link in links:
        if link.startswith("http"):
            continue
        if link.startswith("wikipedia"):
            continue
        iteratively_convert(root_in, root_out, link)


root_in = "/home/random/Development/imagej/imagej/imagej-net-temp/"
root_out = "/home/random/Development/imagej/imagej/imagej.github.io/"

delete_autogenerated_pages(root_out)
iteratively_convert(root_in, root_out, "Development")
iteratively_convert(root_in, root_out, "News")
iteratively_convert(root_in, root_out, "Introduction")
iteratively_convert(root_in, root_out, "Upcoming_Events")
iteratively_convert(root_in, root_out, "Help")
