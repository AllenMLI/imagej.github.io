import os
import markdown_link_extractor
from ij_mw_preprocess import *

autgenerated_line = 'autogenerated: true\n'

weird_pages = ["versioning", "2012-03-20_-_unit_tests_for_imagej_1.46", "deconvolution", "fiji_contribution_requirements"]

def delete_autogenerated_pages(root_out):
    pages_dir = os.path.join(root_out, "pages")
    for filename in os.listdir(pages_dir):
        if filename.endswith(".md"):
            file = os.path.join(pages_dir, filename)
            with open(file, 'r') as f:
                lines = f.readlines()
                if lines[1] == autgenerated_line:
                    print("Deleting " + file)
                    os.remove(file)

def iteratively_convert(root_in, root_out, page_title):
    if "#" in page_title:
        parts = page_title.split("#", 1)
        if parts[0].strip():
            iteratively_convert(root_in, root_out, parts[0])
        return
    if "(" in page_title:
        print("Cannot process names with parentheses: " + page_title)
        return
    if ":" in page_title:
        print("Cannot convert pages with colon in title: " + page_title)
        return
    if page_title.lower() in weird_pages:
        print("Cannot process, pandoc fails: " + page_title)
        return
    path_in = os.path.join(root_in, page_title + ".mw")
    if not os.path.exists(path_in):
        print("Could not find " + page_title)
        return
    path_out = os.path.join(os.path.join(root_out, "pages"), page_title + ".md")
    if os.path.exists(path_out):
        return
    convert_file(path_in, path_out)
    convert_links(path_out, root_in, root_out)

def convert_file(path_in, path_out):
    run_pandoc(path_in, path_out)
    with open(path_out, 'r+') as f:
        lines = f.readlines()
        lines[0] = lines[0] + autgenerated_line
        f.seek(0)
        for line in lines:
            f.write(line)


def convert_links(path_out, root_in, root_out):
    output = read_file(path_out)
    links = markdown_link_extractor.getlinks(output)
    for link in links:
        if link.startswith("http"):
            continue
        if link.startswith("wikipedia"):
            continue
        iteratively_convert(root_in, root_out, link)


root_in = "/home/random/Development/imagej/imagej/imagej-net-temp/"
root_out = "/home/random/Development/imagej/imagej/imagej.github.io/"

delete_autogenerated_pages(root_out)
iteratively_convert(root_in, root_out, "Development")
iteratively_convert(root_in, root_out, "News")
iteratively_convert(root_in, root_out, "Upcoming_Events")