import os
import markdown_link_extractor
from ij_mw_preprocess_debo import *
import re
from shutil import copyfile


def delete_autogenerated_media(root_out):
    media_dir = images_dir(root_out)
    for filename in os.listdir(media_dir):
        file = os.path.join(media_dir, filename)
        os.remove(file)


def images_dir(root_out):
    return os.path.join(os.path.join(root_out, "images"), "pages")


def catch_media(path_in, root_out):
    content = read_file(path_in)
    pattern = re.compile(r'\[\[File\:([^ |]*)[ ]*\|[ ]*[^ |]*[ ]*\|[ ]*link=[^\]]*[ ]*\]\]')
    for (file_name) in re.findall(pattern, content):
        copy_media(file_name, path_in, root_out)
    pattern = re.compile(r'\[\[Image:([^\|\]]*)[^\]]*\]\]')
    for (file_name) in re.findall(pattern, content):
        copy_media(file_name, path_in, root_out)


def copy_media(file_name, path_in, root_out):
    file_out = os.path.join(root_out, file_name)
    file_in = os.path.join(os.path.dirname(path_in), file_name)
    found = True
    if not os.path.exists(file_in):
        print("Could not find media " + file_in)
        found = False
    if os.path.exists(file_out):
        found = False
    if found:
        print("Copying " + file_name)
        copyfile(file_in, file_out)


def delete_autogenerated_pages(root_out):
    pages_dir = os.path.join(root_out, "pages")
    for filename in os.listdir(pages_dir):
        if filename.endswith(".md"):
            file = os.path.join(pages_dir, filename)
            with open(file, 'r') as f:
                lines = f.readlines()
                if lines[1] == autogenerated_line:
                    print("Deleting " + file)
                    os.remove(file)


def _convert(root_in, root_out, page_title, blacklist, recursive=False):
    layout = "page"
    metapage = False
    title = page_title
    if "#" in page_title:
        parts = page_title.split("#", 1)
        if parts[0].strip():
            _convert(root_in, root_out, parts[0], blacklist, recursive)
        return
    if "(" in page_title:
        print("Cannot process names with parentheses: " + page_title)
        return
    if ":" in page_title:
        if "Special:" in page_title:
            # print("Cannot create mediawiki special pages")
            return
        if "File:" in page_title:
            return
        if "Template:" in page_title:
            return
        if "Category:" in page_title:
            layout = "category"
            metapage = True
            page_title = page_title.replace(":Category:", "Category:")
            title = page_title.replace("Category:", "")
        else:
            print("Cannot convert pages with colon in title: " + page_title)
            return
    if page_title in [i[0].strip() for i in blacklist]:
        print("Cannot process, blacklisted: " + page_title)
        return
    path_in = None
    if not metapage:
        path_in = os.path.join(root_in, page_title + ".mw")
        if not os.path.exists(path_in):
            print("Could not find " + page_title)
            return
        title = get_title(path_in)
        catch_media(path_in, images_dir(root_out))
    path_out = os.path.join(os.path.join(root_out, "pages"), page_title + ".md")
    if os.path.exists(path_out):
        return
    convert(path_in, path_out, layout, title)
    if recursive:
        convert_links(path_out, root_in, root_out, blacklist)


def convert_links(path_out, root_in, root_out, blacklist):
    output = read_file(path_out)
    links = markdown_link_extractor.getlinks(output)
    for link in links:
        if link.startswith("http"):
            continue
        if link.startswith("wikipedia"):
            continue
        _convert(root_in, root_out, link, blacklist, True)


def convert_all(root_in, root_out, blacklist):
    for filename in os.listdir(root_in):
        if filename.endswith(".mw"):
            path_in = os.path.join(root_in, filename)
            title = filename.replace(".mw", "")
            # print("trying to convert " + title + "...")
            _convert(root_in, root_out, title, blacklist, recursive=True)


root_in = "/home/random/Development/imagej/imagej/imagej-net-temp/"
root_out = "/home/random/Development/imagej/imagej/imagej.github.io/"
blacklist_path = os.path.join(root_out, "convert/blacklisted-pages.csv")
with open(blacklist_path) as f:
    blacklist = [line.rstrip('\n').split(",") for line in f.readlines()]

delete_autogenerated_pages(root_out)
# _convert(root_in, root_out, "Development", blacklist, recursive=True)
# _convert(root_in, root_out, "News", blacklist, recursive=True)
# _convert(root_in, root_out, "Introduction", blacklist, recursive=True)
# _convert(root_in, root_out, "Upcoming_Events", blacklist, recursive=True)
# _convert(root_in, root_out, "Help", blacklist, recursive=True)
convert_all(root_in, root_out, blacklist)
