{{Infobox
| name                   =  An automated workflow for parallel processing of large multiview SPIM recordings
| software               = Fiji
| author                 = Christopher Schmied, Pavel Tomancak
| maintainer             = Christopher Schmied
| filename               = n.a.
| released               = July 2015
| latest version         = August 2015
| category               = [[:Category:Registration|Registration]]
| website                = https://github.com/mpicbg-scicomp/snakemake-workflows
}}

== Cluster ==

Every cluster is different both in terms of the used hardware and the software running on it, particularly the scheduling system. Here we use a cluster computer at the MPI-CBG that consists of '''44''' nodes each with '''12''' Intel Xeon E5-2640 cores running @ 2.50 GHz and enjoying '''128GB''' of memory. The cluster nodes have access to 200TB of data storage provided by a dedicated Lustre Server architecture, suffice to say that it is optimised for high performance input/output (read/write) operations which is crucial for the SPIM data volumes.

Each node of this cluster runs CentOS 6.3 Linux distribution. The queuing system running on the MPI-CBG cluster is LSF. The basic principles of job submission are the same across queuing systems, but the exact syntax will of course differ.

== Multiview reconstruction ==

The key change in the '''[[Multiview-Reconstruction|Multiview Reconstruction]]''' (MVR) pipeline is that all results are written into an XML. This poses new problems for cluster processing, because several concurrently running jobs need to update the same file. 

Stephan Preibisch solved that problem by allowing to write one XML file per job (usually a timepoint) and then merging the job specific XMLs into one XML for the entire dataset. 

In practice it means the following steps need to be executed:

* Define XML dataset - creates one XML for the entire timelapse
* Re-save data as HDF5 - converts data into HDF5 container optimised for fast access in [[BigDataViewer]] 
* Run per time-point registrations - creates as many XMLs as there are timepoints
* Merge XMLs - consolidates the per-timepoint XMLs back into a single XML

Some new parameters are introduced and some old parameters change names. Therefore, use the master file described in this chapter to process with the MVR pipeline.


== Pre-requisites ==

=== Supported datasets ===

The scripts are now supporting multiple angles, multiple channels and multiple illumination direction without adjusting the Snakefile or .bsh scripts.

Using spimdata version: 0.9-revision

Using SPIM registration version 3.3.9

Supported datasets are in the following format:

Using Zeiss Lightsheet Z.1 Dataset (LOCI)

    Multiple timepoints:  YES (one file per timepoint) or (all time-points in one file)
    Multiple channels:  YES (one file per channel) or (all channels in one file)
    Multiple illumination directions: YES (one file per illumination direction)
    Multiple angles: YES (one file per angle)
    
Using LOCI Bioformats opener (.tif)

    Multiple timepoints: YES (one file per timepoint) or (all time-points in one file)
    Multiple channels: YES (one file per channel) or (all channels in one file)
    Multiple illumination directions: YES (one file per illumination direction) => not tested yet
    Multiple angles: YES (one file per angle)
    
Using ImageJ Opener (.tif):

    Multiple timepoints: YES (one file per timepoint)
    Multiple channels: YES (one file per channel)
    Multiple illumination directions: YES (one file per illumination direction) => not tested yet
    Multiple angles: YES (one file per angle)

==== Example data ====

You can download a 5 view, single channel .czi example dataset here:

http://tomancak-srv1.mpi-cbg.de/~schmied/

=== Command line ===

It is very likely that the cluster computer does not run ANY Graphical User Interface and relies exclusively on the command line. Steering a cluster from the command line is fairly easy - I use about 10 different commands to do everything I need to do. Since the Linux command line may be unfamiliar to most biologists we start a separate [[Linux_command_line_tutorial]] page that explains the bare essentials.

=== Snakemake for workflow ===

[https://bitbucket.org/johanneskoester/snakemake/wiki/Home Snakemake] (command-line workflow engine) is used to automatically execute individual steps in the workflow. The workflow documented on this page was tested with snakemake 3.3 (interfaced with [www.pyyaml.org PyYAML](version 3.11) and [https://github.com/pygridtools/drmaa-python python drmaa](version 0.7.6) support.

=== Fiji for workflow ===

You can a download Fiji version that we have tested for compatibility with the automated cluster processing here:

http://tomancak-srv1.mpi-cbg.de/~schmied/

We will from time to time upgrade.

=== sysconfcpus ===

We use Libsysconfcpus (http://www.kev.pulo.com.au/libsysconfcpus/) to restrict how many cores Fiji is using on the cluster.

Compile with:
<source lang="bash">
 CFLAGS=-ansi ./configure --prefix=$PREFIX
 make
 make install
</source>

where PREFIX is the installation directory.
ANSI mode is necessary when compiling with our default GCC version, 4.9.2.
It may or may not be necessary with older versions.

=== Setup of workflow ===

Clone the repository:

The repository contains the example configuration scripts for single and dual channel datasets, the Snakefile which defines the workflow, the beanshell scripts which drive the processing via Fiji and a cluster.json file which contains information for the cluster queuing system. 

<source lang="bash">
/path/to/repository/spim_registration/timelapse/
├── config.yaml
├── Snakefile
├── cluster.json
├── define_tif_zip.bsh
├── define_czi.bsh
├── registration.bsh
├── deconvolution.bsh
├── transform.bsh		
├── registration.bsh 	
└── xml_merge.bsh	
 		
</source>

A data directory e.g. looks like this:

It contains the .yaml file for the specific dataset. You can either copy it, if you want to keep it together with the dataset, or make a symlink from the processing repository. 

<source lang="bash">
/path/to/data/
├── dataset.czi
├── dataset(1).czi
├── dataset(2).czi
├── dataset(3).czi
├── dataset(4).czi
└── dataset.yaml	 		# copied/symlinked from this repo

</source>


* `tomancak.yaml` contains the parameters that configure the beanshell scripts found in the data directory
* `Snakefile` from this directory
* `cluster.json` that resides in the same directory as the `Snakefile`
* cluster runs LSF

=== Tools ===


The tool directory contains scripts for common file format pre-processing.
Some datasets are currently only usable when resaving them into .tif:
* discontinous .czi datasets
* .czi dataset with multiple groups

The master_preprocesing.sh file is the configuration script that contains the information about the dataset that needs to be resaved. In the czi_resave directory you will find the the create-resaving-jobs.sh script that creates a job for each TP. The submit-jobs script sends these jobs to the cluster were they call the resaving.bsh script. The beanshell then uses executes the Fiji macro and resaves the files. The resaving of czi files is using LOCI bioformats and preserves the metadata. 

<source lang="bash">
/path/to/repository/spim_registration/tools/
├── master_preprocessing.sh
├── czi_resave/
    ├── create-resaving-jobs.sh
    ├── resaving.bsh
    └── submit-jobs
</source>

== workflow ==


The current workflow consists of the following steps. It covers the prinicipal processing for timelapse multiview SPIM processing:

* define czi or tif dataset
* resave into hdf5
* detect and register interespoints
* merge xml
* timelapse registration
* optional for dual channel dataset: dublicate transformations
* optional for deconvolution: external transformation
* average-weight fusion/deconvolution
* define output
* resave output into hdf5


=== Preparations for processing ===

The entire processing is controlled via the yaml file.

In the first part (common) of the yaml file the key parameters for the processing are found.
These parameters are usually dataset and user dependent.
The second part contains the advanced and manual overrides for each processing step. These steps correspond to the rules in the snakefile.



1. Processing switches

1.1. Switch between all channels contain beads and one channel of two contains beads
     
1.2. Switch between fusion and deconvolution 

    
2. Define dataset
    
2.1. General Settings

2.2. Settings for .czi files
    
2.3 Settings for .tif datasets

    
3. Detection and registration


4. Timelapse registration


5. Weighted-average fusion


6. Multiview deconvolution

6.1. External transformation
    
6.2. Deconvolution settings 


7. Software directories


8. Fiji Resource settings


9. Advanced settings 

9.1. define_xml_czi
    
9.2. define_xml_tif
    
9.3. resave_hdf5
    
9.4. registration
    
9.5. xml_merge
    
9.6. timelapse
    
9.7. dublicate_transformations
    
9.8. fusion
    
9.9. external_transform
    
9.10. deconvolution
    
9.11. hdf5_output
    

<source lang="bash">
common: {
  # ============================================================================
  # ============================================================================
  # yaml example file 
  #
  # DESCRIPTION: source file for cluster processing scripts
  #
  #      AUTHOR: Christopher Schmied, schmied@mpi-cbg.de
  #   INSTITUTE: Max Planck Institute for Molecular Cell Biology and Genetics
  #        BUGS:
  #       NOTES:
  #     Version: 3.3
  #     CREATED: 2015-06-01
  #    REVISION: 2015-07-19
  # ============================================================================
  # ============================================================================
  # 1. Software directories
  # 
  # Description: paths to software dependencies of processing
  # Options: Fiji location
  #          beanshell and snakefile diretory
  #          directory for cuda libraries
  #          xvfb setting
  #          sysconfcpus setting
  # ============================================================================
  # current working Fiji
  fiji-app: "/sw/users/schmied/packages/2015-06-30_Fiji.app.cuda/ImageJ-linux64",
  # bean shell scripts and Snakefile
  bsh_directory: "/projects/pilot_spim/Christopher/snakemake-workflows/spim_registration/timelapse/",
  # Directory that contains the cuda libraries
  directory_cuda: "/sw/users/schmied/cuda/",
  # xvfb 
  fiji-prefix: "/sw/users/schmied/packages/xvfb-run -a",       # calls xvfb for Fiji headless mode
  sysconfcpus: "sysconfcpus -n",
  # ============================================================================
  # 2. Processing switches
  #
  # Description: Use switches to decide which processing steps you need:
  # Options:  transformation_switch: "timelapse",
  #           goes directly into fusion after timelapse registration
  #
  #           transformation_switch: "timelapse_duplicate",
  #           for dual channel processing one channel contains the beads
  #           dublicates the transformation from the source channel to the 
  #           target channel
  #
  #           Switches between content based fusion and deconvoltion
  #           fusion_switch: "deconvolution", > for deconvolution
  #           fusion_switch: "fusion", > for content based fusion
  # ============================================================================
  # Transformation switch:
  transformation_switch: "timelapse",
  # Fusion switch:
  fusion_switch: "deconvolution",
  # ============================================================================
  # 3. Define dataset
  #
  # Description: key parameters for processing
  # Options: General Settings
  #          Settings for .czi files
  #          Settings for .tif datasets
  # ============================================================================
  # 3.1. General Settings -------------------------------------------------------
  #
  # Description: applies to both .czi and tif datasets
  # Options: xml file name
  #          number of timepoints
  #          angles
  #          channels
  #          illuminations
  # ----------------------------------------------------------------------------
  hdf5_xml_filename: '"dataset_one"', 
  ntimepoints: 90,        # number of timepoints of dataset
  angles: "0,72,144,216,288",   # format e.g.: "0,72,144,216,288",
  channels: "green",     # format e.g.: "green,red", IMPORTANT: for tif numeric!
  illumination: "0",     # format e.g.: "0,1",
  #
  # 3.2. Settings for .czi files -----------------------------------------------
  #
  # Description: applies only to .czi dataset
  # Options: name of first czi file
  # ----------------------------------------------------------------------------
  first_czi: "2015-04-21_LZ2_Stock32.czi", 
  #
  # 3.3. Settings for .tif datasets --------------------------------------------
  #
  # Description: applies only to .tif dataset
  # Options: file pattern of .tif files:
  #          multi channel with one file per channel: 
  #          spim_TL{tt}_Angle{a}_Channel{c}.tif
  #          for padded zeros use tt 
  # ----------------------------------------------------------------------------
  image_file_pattern: 'img_TL{{t}}_Angle{{a}}.tif',
  multiple_channels: '"NO (one channel)"',         # '"YES (all channels in one file)"' or '"YES (one file per channel)"' or '"NO (one channel)"'
  # ============================================================================
  # 4. Detection and registration
  #
  # Description: settings for interest point detection and registration
  # Options: Single channel and dual channel processing
  #          Source and traget for dual channel one channel contains the beads
  #          Interestpoints label
  #          Difference-of-mean or difference-of-gaussian detection
  # ============================================================================
  # reg_process_channel:
  # Single Channel: '"All channels"'
  # Dual Channel: '"All channels"'
  # Dual Channel one Channel contains beads: '"Single channel (Select from List)"'
  reg_process_channel: '"All channels"',
  #
  # Dual channel 1 Channel contains the beads: which channel contains the beads?
  # Ignore if Single Channel or Dual Channel both channels contain beads
  source_channel: "red", # channel that contains the beads
  target_channel: "green", # channel without beads
  # reg_interest_points_channel:
  # Single Channel: '"beads"'
  # Dual Channel: '"beads,beads"'
  # Dual Channel: Channel does not contain the beads '"[DO NOT register this channel],beads"'
  reg_interest_points_channel: '"beads"',
  #
  # type of detection: '"Difference-of-Mean (Integral image based)"' or '"Difference-of-Gaussian"'
  type_of_detection: '"Difference-of-Gaussian"',
  # Settings for Difference-of-Mean
  # For multiple channels 'value1,value2' delimiter is ,
  reg_radius_1: '2',
  reg_radius_2: '3',
  reg_threshold: '0.005',
  # Settings for Difference-of-Gaussian
  # For multiple channels 'value1,value2' delimiter is ,
  sigma: '1.3',
  threshold_gaussian: '0.025',
  # ============================================================================
  # 5. Timelapse registration
  #
  # Description: settings for timelapse registration
  # Options: reference timepoint
  # ============================================================================
  reference_timepoint: '45',   # Reference timepoint
  # ============================================================================
  # 6. Weighted-average fusion
  #
  # Description: settings for content-based multiview fusion
  # Options: downsampling
  #          Cropping parameters based on full resolution
  # ============================================================================
  downsample: '1',    # set downsampling
  minimal_x: '274',   # Cropping parameters of full resolution
  minimal_y: '17',
  minimal_z: '-423',
  maximal_x: '1055',
  maximal_y: '1928',
  maximal_z: '480',
  # ============================================================================
  # 7. Multiview deconvolution
  #
  # Description: settings for multiview deconvolution
  # Options: External transformation
  #          Deconvolution settings
  #
  # ============================================================================
  # 7.1. External transformation -----------------------------------------------
  #
  # Description: Allows downsampling prior deconvolution
  # Options: no downsampling: 
  #          external_trafo_switch: "_transform",
  #
  #          downsampling:
  #          external_trafo_switch: "external_trafo",
  #          IMPORTANT: boundingbox needs to reflect this downsampling. 
  #
  #          Matrix for downsampling
  # ----------------------------------------------------------------------------
  external_trafo_switch: "_transform",
  #
  # Matrix for downsampling
  matrix_transform: '"0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0"',
  #
  # 7.2. Deconvolution settings ------------------------------------------------
  # 
  # Description: core settings for multiview deconvolution
  # Options: number of iterations
  #          Cropping parameters taking downsampling into account!
  #          Channel settings for deconvolution
  # ----------------------------------------------------------------------------
  iterations: '15',        # number of iterations
  minimal_x_deco: '137',  # Cropping parameters: take downsampling into account
  minimal_y_deco: '-8',
  minimal_z_deco: '-211',
  maximal_x_deco: '527',
  maximal_y_deco: '964',
  maximal_z_deco: '240',
  # Channel settings for deconvolution
  # Single Channel: '"beads"'
  # Dual Channel: '"beads,beads"'
  # Dual Channel one channel contains beads: '"[Same PSF as channel red],beads"'
  detections_to_extract_psf_for_channel: '"beads"'
  }
</source>

=== Submitting Jobs === 

We recommend to execute snakemake within screen. 
To execute snakemake you need to call snakemake, specify the number of jobs, the location of the data and to dispatch jobs to a cluster the information for the queuing system. Here is a list of commands and flags that are used for the snakemake workflow:

Local back end:
/path/to/snakemake/snakemake -j 1 -d /path/to/data/

Flag for number of jobs run in parallel:
-j <number of jobs>

Flag for specifying data location:
-d /path/to/data/

Flag for dry run of snakemake:
-n

Force the execution of a rule:
-R <name of rule>

For DRMAA back end add: 
--drmaa " -q {cluster.lsf_q} {cluster.lsf_extra}"

For Lsf backend add:
--cluster "bsub -q {cluster.lsf_q} {cluster.lsf_extra}”

To specify the configuration script for the queuing system:
--cluster-config ./cluster.json

To save error and output files of cluster add:
--drmaa " -q {cluster.lsf_q} {cluster.lsf_extra} -o test.out -e test.err"
--cluster "bsub -q {cluster.lsf_q} {cluster.lsf_extra} -o test.out -e test.err“



The commands to execute snakemake would then look like this:

If DRMAA is supported on your cluster:

<source lang="bash">
/path/to/snakemake/snakemake -j2 -d /path/to/data/ --cluster-config ./cluster.json --drmaa " -q {cluster.lsf_q} {cluster.lsf_extra}"
</source>

If not:

<source lang="bash">
/path/to/snakemake/snakemake -j2 -d /path/to/data/ --cluster-config ./cluster.json --cluster "bsub -q {cluster.lsf_q} {cluster.lsf_extra}"
</source>

For error and output of the cluser add -o test.out -e test.err e.g.:

DRMAA
<source lang="bash">
/path/to/snakemake/snakemake -j2 -d /path/to/data/ --cluster-config ./cluster.json --drmaa " -q {cluster.lsf_q} {cluster.lsf_extra} -o test.out -e test.err"
</source>

LSF
<source lang="bash">
/path/to/snakemake/snakemake -j2 -d /path/to/data/ --cluster-config ./cluster.json --cluster "bsub -q {cluster.lsf_q} {cluster.lsf_extra} -o test.out -e test.err"
</source>

Note:  the error and output of the cluster of all jobs are written into these files.

=== Log files and supervision of the pipeline ===


The log files are written into a new directory in the data directory called "logs".
The log files are ordered according to their position in the workflow. Multiple or alternative steps in the pipeline are indicated by numbers. 

force certain rules:
use the -R flag to rerun a particular rule and everything downstream
-R <name of rule>
