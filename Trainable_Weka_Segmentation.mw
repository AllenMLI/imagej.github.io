{{Infobox Plugin
| name                   = Advanced Weka Segmentation
| software               = Fiji
| author                 = [[User:Iarganda | Ignacio Arganda-Carreras]], [http://albert.rierol.net Albert Cardona], [http://www.kaynig.de Verena Kaynig]
| maintainer             = [[User:Iarganda | Ignacio Arganda-Carreras]]
| source                 = [http://pacific.mpi-cbg.de/cgi-bin/gitweb.cgi?p=fiji.git;a=tree;f=src-plugins/Trainable_Segmentation/trainableSegmentation;h=5e5ffbd4ddffda72abd1d29099a2b1be523cdc1c;hb=HEAD on gitweb]
| released               = May 19<sup>th</sup>, 2011
| latest version         = January 17<sup>th</sup>, 2012
| status                 = '''BETA''', active
| category               = [[:Category:Segmentation|Segmentation]], [[:Category:Machine Learning|Machine Learning]]
}}

'''Advanced''': this plugin has many more features and options than the basic and easier to use [[Trainable Segmentation]] plugin.

'''Weka''': it makes use of all the powerful tools and classifiers from the latest version of [http://www.cs.waikato.ac.nz/ml/weka/ Weka].

'''Segmentation''': it provides a labeled result based on the training of a chosen classifier.

== Introduction ==
The [[Advanced Weka Segmentation]] is a Fiji plugin that combines a collection of machine learning algorithms with a set of selected image features to produce pixel-based segmentations. [http://www.cs.waikato.ac.nz/ml/weka/ Weka] (Waikato Environment for Knowledge Analysis) can itself be called from the plugin. It contains a collection of visualization tools and algorithms for data analysis and predictive modeling, together with graphical user interfaces for easy access to this functionality. As described on their wikipedia site, the advantages of [http://www.cs.waikato.ac.nz/ml/weka/ Weka] include: 

* freely availability under the [http://www.gnu.org/licenses/gpl.txt GNU General Public License]

* portability, since it is fully implemented in the Java programming language and thus runs on almost any modern computing platform 

* a comprehensive collection of data preprocessing and modeling techniques 

* ease of use due to its graphical user interfaces

[http://www.cs.waikato.ac.nz/ml/weka/ Weka] supports several standard [http://en.wikipedia.org/wiki/Data_mining data mining] tasks, more specifically, data preprocessing, [http://en.wikipedia.org/wiki/Cluster_analysis clustering], [http://en.wikipedia.org/wiki/Classification_(machine_learning) classification], [http://en.wikipedia.org/wiki/Regression_analysis regression], visualization, and [http://en.wikipedia.org/wiki/Feature_selection feature selection].

The main goal of this plugin is to work as a '''bridge between the Machine Learning and the Image Processing''' fields. It provides the framework to use and, more important, '''compare''' any available classifier to perform image segmentation based on pixel classification.

== The Graphical User Interface ==
[[Image:Advanced-Weka-Segmentation-GUI.png|thumb|400px|Example of the first look of the plugin window when using it on a TEM image]] [[Advanced Weka Segmentation]] runs on any 2D image or 2D stack of images (of any type).


By default, the plugin starts with two classes, i.e. it will produce '''binary pixel classification'''. The user can add traces to both classes using the whole set of [http://rsbweb.nih.gov/ij/docs/guide/userguide-19b.html#toc-Section-19 tools for ROI] (region of interest) drawing available in Fiji. That includes rectangular, round rectangular, oval, elliptical, brush polygon and freehand selections. By default, the freehand selection tool (of 1 pixel width) is automatically selected. 

The user can pan, zoom in and out, or scroll between slices (if the input image is a stack) in the main canvas as if it were any other Fiji window. On the left side of the canvas there are two panels of buttons, one for the training and one for the general options. On the right side of the image canvas we have a panel with the list of traces for each class and a button to add the current ROI to that specific class. All buttons contain a short explanation of their functionality that is displayed when the cursor lingers over the buttons.

=== Training panel ===
[[Image:AWS-GUI-after-training.png|thumb|400px|Example of the aspect of the plugin window after training on a TEM image]]
==== Train classifier ====
This button activates the training process. One trace of two classes is the minimum required to start training. The first time this button is pressed, the features of the input image will be extracted and converted to a set of vectors of float values, which is the format the [http://www.cs.waikato.ac.nz/ml/weka/ Weka] classifiers are expecting. This step can take some time depending on the size of the images, the number of features and the number of cores of the machine where Fiji is running. The feature calculation is done in a completely multi-thread fashion. The features will be only calculated the first time we train after starting the plugin or after changing any of the feature options.

If the training ends correctly, then the displayed image will be completely segmented and the result will be overlayed with the corresponding class colors.  Notice that all buttons are now enabled, since all their functionalities are possible after training.

'''While training, this button will show the label "STOP"'''. By clicking on it, the whole training process will be interrupted and the plugin reseted to the state previous to the training.

==== Toggle overlay ====
[[Image:AWS-Probability-maps.png|thumb|250px|Example of resulting probability map displayed as a hyperstack]]
This button activates and deactivates the overlay of the result image. The transparency of the overlay image can be adjusted in the [[Advanced_Weka_Segmentation#Settings|Settings dialog]].

==== Create result ====

It creates and displays the resulting image. This image is equivalent to the current overlay but grayscale. By default, the result image will be binary (black and white).

==== Get probability ====

Based on the current classifier, the probability that each pixel belongs to each class is displayed on a 32-bit hyperstack.

==== Plot result ====

This button calls the [http://www.cs.waikato.ac.nz/ml/weka/ Weka] code to generate the model performance chart, i.e. the [http://en.wikipedia.org/wiki/Receiver_operating_characteristic ROC], [http://en.wikipedia.org/wiki/Precision_and_recall precision/recall], etc. curves based on the training data. 

These curves allow to visualize the performance of the classifier based on the different thresholds that can be applied to the probability maps.

[[Image:AWS-Plot-result.png|thumb|300px|Weka model performance chart. Displayed after clicking on "Plot result"]]

=== Options panel ===

==== Apply classifier ====

By clicking on this button we can apply the current classifier to any 2D image or stack of images we have in our file system. Two dialogs will pop up to, first, ask the user for the input image or stack and, second, ask if the result should be displayed '''as a probability map or a segmentation''' (final classes). Then the plugin will perform the image segmentation based on the current classifier and ---consequently--- selected features. This may take a while depending on the number and size of the input images and the number of cores of the machine. After finishing, the input image (or stack) and its corresponding segmentation will be displayed.

==== Load classifier ====

Here we can load any previously saved classifier. The plugin will check and adjust the selected features with the attributes of this new classifier. The classifier file format is the one used in [http://www.cs.waikato.ac.nz/ml/weka/ Weka] (.model).

==== Save classifier ====

It saves the current classifier into a file, under the standard [http://www.cs.waikato.ac.nz/ml/weka/ Weka] format (.model). This allows us to store classifiers and apply them later on different sessions (option not available in the simple [[Trainable Segmentation]] plugin).

==== Load data ====

Here we can load the data (in [http://www.cs.waikato.ac.nz/ml/weka/ Weka] format) from previous traces on the same or other image or stack. Again, the plugin will check and force the consistency between the loaded data and the current image, features and classes. The input file format is the standard Weka format: [http://weka.wikispaces.com/ARFF+%28stable+version%29 ARFF]. Notice the traces (regions of interests selected by the user) are not saved but their corresponding feature vectors.

==== Save data ====

With this button we can save the current trace information into a data file that we can handle later with the plugin or the Weka Explorer itself. The plugin will save the feature vectors derived from the pixels belonging to each trace into an [http://weka.wikispaces.com/ARFF+%28stable+version%29 ARFF] file at a location chosen by the user.

==== Create new class ====

The default number of classes of the plugin is two, but through this button we can increase up to 5 the classes. The name of the new classes can be changed on the [[Advanced_Weka_Segmentation#Settings|Settings dialog]].

==== Settings ====
[[Image:AWS-Settings.png|thumb|200px|Settings dialog of the Advanced Segmentation Plugin]]
The rest of tunable parameters of the plugin can be changed on the Settings dialog, which is displayed when clicking on this button.

===== Training features =====
Here we can select and deselect the training features, which are the key of the learning procedure. The plugin creates a stack of images ---one image for each feature. For instance, if only [http://en.wikipedia.org/wiki/Gaussian_blur Gaussian blur] is selected as a feature, the classifier will be trained on the original image and some blurred versions to it with different <math>\sigma</math> parameters for the Gaussian. 

If the input image is grayscale, the features will be calculated using double precision (32-bit images). In the case of RGB input images, the features will be RGB as well.

The different available features are:
* '''Gaussian blur''': performs <math>n</math> individual convolutions with Gaussian kernels with <math>\sigma</math> equal to <math>1, 2, 4... 2^{n}</math>. The larger the radius the more blurred the image becomes until the pixels are homogeneous (by default <math>n=16</math>).
* '''Sobel filter''': calculates the [http://en.wikipedia.org/wiki/Sobel_filter gradient at each pixel]. [http://en.wikipedia.org/wiki/Gaussian_blur Gaussian blurs] with <math>\sigma=1, 2, 4... 2^{n}</math> are performed prior to the filter.
* '''Hessian''': Calculates a [http://en.wikipedia.org/wiki/Hessian_matrix Hessian matrix] <math>H</math> at each pixel. Prior to the application of any filters, a [http://en.wikipedia.org/wiki/Gaussian_blur Gaussian blur] with <math>\sigma=1, 2,
4... 2^{n}</math> is performed. The final features used for pixel classification, given the Hessian matrix <math>\left(\begin{array}{cc}
a & b\\
c & d\end{array}\right)</math> are calculated thus:
** Module: <math>\sqrt{a^{2}+bc+d^{2}}</math>.
** Trace: <math>a+d</math>.
** Determinant: <math>ad-cb</math>.
** First eigenvalue: <math>\frac{a+d}{2}+\sqrt{\tfrac{4b^{2}+(a-d)^{2}}{2}}</math>.
** Second eigenvalue: <math>\frac{a+d}{2}-\sqrt{\tfrac{4b^{2}+(a-d)^{2}}{2}}</math>.
** Orientation: <math>\frac{1}{2}\arccos\left(4b^{2}+(a-d)^{2}\right)</math> This operation returns the orientation for which the second derivative is maximal. It is an angle returned in radians in the range <math>\left[-\frac{\pi}{2},\frac{\pi}{2}\right]</math> and corresponds to an orientation without direction. The orientation for the minimal second derivative can be obtained by adding (or subtracting) <math>\frac{\pi}{2}</math>.
** Gamma-normalized square eigenvalue difference: <math>t^{4}(a-d)^{2}\left((a-d)^{2}+4b^{2}\right)</math>, where <math>t=1^{3/4}</math>.
** Square of Gamma-normalized eigenvalue difference: <math>t^{2}\left((a-d)^{2}+4b^{2}\right)</math>, where <math>t=1^{3/4}</math>.
* '''Difference of gaussians''': calculates two [http://en.wikipedia.org/wiki/Gaussian_blur Gaussian blur] images from the original image and subtracts one from the other. <math>\sigma</math> values are again set to <math>1, 2, 4... 2^{n}</math> , so <math>(n-1)!</math> feature images are added to the stack.
* '''Membrane projections''': the initial kernel for this operation is hardcoded as a <math>19x19</math> zero matrix with the middle column entries set to 1. Multiple kernels are created by rotating the original kernel by 6 degrees up to a total rotation of 180 degrees, giving 30 kernels. Each kernel is convolved with the image and then the set of 30 images are Z-projected into a single image via 6 methods:
** sum of the pixels in each image
** mean of the pixels in each image
** standard deviation of the pixels in each image
** median of the pixels in each image
** maximum of the pixels in each image
** minimum of the pixels in each image
: Each of the 6 resulting images is a feature. Hence pixels in lines of similarly valued pixels in the image that are different from the average image intensity will stand out in the Z-projections.
* '''Mean, Variance, Median, Minimum, Maximum''': the pixels within a radius of <math>1, 2, 4... 2^{n}</math> pixels from the target pixel are subjected to the pertinent operation (mean/min etc.) and the target pixel is set to that value.
* '''Anisotropic diffusion''': the [[Anisotropic_Diffusion_2D | anisotropic diffusion filtering]] from Fiji with 20 iterations, <math>1, 2, 4... 2^{n}</math> smoothing per iterations, <math>a_{1}=0.10, 0.35</math>, <math>a_{2}=0.9</math>, and an edge threshold set to the membrane size.
* '''Bilateral filter''': is very similar to the Mean filter but better preserves edges while averaging/blurring other parts of the image. The filter accomplishes this task by only averaging the values around the current pixel that are close in color value to the current pixel. The 'closeness' of other neighborhood pixels to the current pixels is determined by the specified threshold. I.e. for a value of 10 each pixel that contributes to the current mean have to be within 10 values of the current pixel. In our case, we combine spatial radius of 5, 10 and 20, with a range radius of 50 and 100.
* '''Lipschitz filter''': from [http://rsbweb.nih.gov/ij/plugins/lipschitz/ Mikulas Stencel plugin]. This plugin implements Lipschitz cover of an image that is equivalent to a grayscale opening by a cone. The Lipschitz cover can be applied for the elimination of a slowly varying image background by subtraction of the lower Lipschitz cover (a top-hat procedure). Sequential double scan algorithm (according Rosenfeld) was used. We use down and top hats, with slope = 5, 10, 15, 20, 25.
* '''Kuwahara filter''': another noise-reduction filter that preserves edges. This is a version of the [[Linear_Kuwahara | Kuwahara filter that uses linear kernels]] rather than square ones. We use the membrane patch size as kernel size, 30 angles and 0, 1 and 2.
* '''Gabor filter''': at the moment this option may take some time and memory because it generates a very diverse range of [http://en.wikipedia.org/wiki/Gabor_filter Gabor filters] ('''45'''). ''' This may undergo changes in the future'''. The implementation details are included in this [[Gabor Filter script|script]].
* '''Derivatives filter''': calculates high order derivatives of the input image (from the second to the 5th derivative) using [[FeatureJ]].
* '''Laplacian filter''': computes the Laplacian of the input image using [[FeatureJ]]. It uses smoothing scale <math>\sigma=1, 2, 4... 2^{n}</math>.
* '''Structure filter''': calculates for all elements in the input image, the eigenvalues (smallest and largest) of the so-called structure tensor using [[FeatureJ]]. It uses smoothing scale <math>\sigma=1, 2, 4... 2^{n}</math> and integration scales 1 and 3.

When using grayscale images, the input image will be also included as a feature. In the case of color (RGB) images, the '''Hue, Saturation and Brightness''' will be as well part of the features.

===== Feature options =====

* '''Membrane thickness''': expected value of the membrane thickness, 1 pixel by default. The more accurate, the more precise the filter will be.
* '''Membrane patch size''': this represents the size NxN of the field of view for the membrane projection filters.
* '''Minimum sigma''': minimum radius of the filters used to create the features. By default 1 pixel.
* '''Maximum sigma''': maximum radius of the filters used to create the features. By default 16 pixels.

===== Classifier options =====
[[Image:AWS-Classifier-selection.png|thumb|200px|Classifier selection in the Advanced Weka Segmentation [[Advanced_Weka_Segmentation#Settings|Settings dialog]].]]
The default classifier is a '''multi-threaded''' version of  [http://en.wikipedia.org/wiki/Random_forest random forest] with 200 trees and 2 random features per node. However the user can select any available classifier in the [http://www.cs.waikato.ac.nz/ml/weka/ Weka] by clicking on "Choose" button. By left-clicking on the classifier text we can also edit the classifier options.

===== Class names =====
The classes can be renamed using these text boxes.

===== Homogenize classes =====
The classsifier uses by the default all the user traces to train. By clicking on this option, we filter first the classes in order to provide a homogeneous distribution of the samples. This implies that the less numerous classes will duplicate some of their samples and the more populated classes will lose some of their samples for the sake of even distribution. This option is hardly recommended if we want to give the same importance to all classes. An alternative is to use the [http://weka.wikispaces.com/CostSensitiveClassifier Weka CostSensitiveClassifier] and set a corresponding cost matrix.

===== Save feature stack =====
We can save the features as a stack of images by clicking on this button. It will use the last feature configuration that is available.

===== Result overlay opacity =====
This slider sets the opacity of the resulting overlay image. Depending on the image contrast of our input images, we might be interested on adjusting this value.

==== WEKA ====

The Weka button launches the Weka GUI Chooser, where we can start all the applications available in [http://www.cs.waikato.ac.nz/ml/weka/ Weka]: 

* '''Explorer''': an environment for exploring data with [http://www.cs.waikato.ac.nz/ml/weka/ Weka].
* '''Experimenter''': an environment for performing experiments and conducting statistical tests between learning schemes.
* '''KnowledgeFlow''': this environment supports essentially the same functions as the Explorer but with a drag-and-drop interface. One advantage is that it supports incremental learning.
* '''SimpleCLI''': provides a simple command-line interface that allows direct execution of [http://www.cs.waikato.ac.nz/ml/weka/ Weka] commands for operating systems that do not provide their own command line interface.

For a complete step-by-step description on how to compare classifiers for image segmentation using the [http://www.cs.waikato.ac.nz/ml/weka/ Weka] Explorer, have a look at the [[Advanced Weka Segmentation - How to compare classifiers‎]] tutorial.

=== Macro language compatibility ===
[[Advanced Weka Segmentation]] is completely compatible with the popular [http://rsb.info.nih.gov/ij/developer/macro/macros.html ImageJ macro language]. Each of the buttons in the GUI are macro-recordable and their commands can be reproduced later from a simple macro file.
[[Image:AWS-macro-recording.png|thumb|760px|Example of macro recording of the Advanced Weka Segmentation tools.]]

The complete list of commands is as follows:
* Start the plugin:
<source lang="java"> 
run("Advanced Weka Segmentation"); 
</source>
* Add traces (current ROI) to a class:
:Format: <code>addTrace( class index, slice number )</code>

:For example, to add the selected ROI of the first slice to the first class, we type:
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.addTrace", "0", "1");
</source>
* Train classifier: 
<source lang="java">call("trainableSegmentation.Weka_Segmentation.trainClassifier");
</source>
* Toggle overlay:
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.toggleOverlay");
</source>
* Create binary result:
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.getResult");
</source>
* Get probability maps:
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.getProbability");
</source>
* Plot the threshold curves:
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.plotResultGraphs");
</source>
* Apply the current classifier to an image or stack:
:Format: <code>applyClassifier( input directory, input image or stack, show results flag, store results flag, probability maps flag, store folder)</code>

:Example:
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.applyClassifier",
"/home/iarganda/data/", "input-image.tif", "showResults=true", 
"storeResults=false", "probabilityMaps=false", "");
</source>
* Load a classifier from file:
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.loadClassifier",
 "/home/iarganda/classifier.model");
</source>
* Save classifier into a file:
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.saveClassifier",
 "/home/iarganda/classifier.model");
</source>
* Load data from an ARFF file:
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.loadData", "/home/iarganda/data.arff");
</source>
* Save current data into a file:
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.saveData", "/home/iarganda/data.arff");
</source>
* Create new class:
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.createNewClass", "new-class-name");
</source>
* Launch Weka:
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.launchWeka");
</source>
* Enable/disable a specific feature:
:Format: <code>setFeature( "feature name=true or false" )</code>
:Example (enable Variance filters):
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.setFeature", "Variance=true");
</source>
* Change a class name:
:Format: <code>changeClassName( class index, class new name )</code>
:Example (change first class name to "background"):
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.changeClassName", "0", "background");
</source>
* Set option to balance the class distributions:
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.setClassHomogenization", "true");
</source>
* Set membrane thickness (in pixels):
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.setMembraneThickness", "2");
</source>
* Set the membrane patch size (in pixels, NxN):
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.setMembranePatchSize", "16");
</source>
* Set the minimum kernel radius (in pixels):
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.setMinimumSigma", "2.0");
</source>
* Set the maximum kernel radius (in pixels):
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.setMaximumSigma", "8.0");
</source>
* Set a new classifier:
:Format: <code>setClassifier( classifier class, classifier options )</code>
:Example (change classifier to NaiveBayes):
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.setClassifier",
"weka.classifiers.bayes.NaiveBayes", "");
</source>
* Set the result overlay opacity:
<source lang="java">
call("trainableSegmentation.Weka_Segmentation.setOpacity", "50");
</source>
=====Complete macro example:=====
<source lang="java">
// Open Leaf sample
run("Leaf (36K)");

// start plugin
run("Advanced Weka Segmentation");

// wait for the plugin to load
wait(3000);
selectWindow("Advanced Weka Segmentation");

// add one region of interest to each class
makeRectangle(367, 0, 26, 94);
call("trainableSegmentation.Weka_Segmentation.addTrace", "0", "1");
makeRectangle(186, 132, 23, 166);
call("trainableSegmentation.Weka_Segmentation.addTrace", "1", "1");

// enable some extra features
call("trainableSegmentation.Weka_Segmentation.setFeature", "Variance=true");
call("trainableSegmentation.Weka_Segmentation.setFeature", "Mean=true");
call("trainableSegmentation.Weka_Segmentation.setFeature", "Minimum=true");
call("trainableSegmentation.Weka_Segmentation.setFeature", "Maximum=true");
call("trainableSegmentation.Weka_Segmentation.setFeature", "Median=true");

// change class names
call("trainableSegmentation.Weka_Segmentation.changeClassName", "0", "background");
call("trainableSegmentation.Weka_Segmentation.changeClassName", "1", "leaf");

// balance class distributions
call("trainableSegmentation.Weka_Segmentation.setClassHomogenization", "true");

// train current classifier
call("trainableSegmentation.Weka_Segmentation.trainClassifier");

// display probability maps
call("trainableSegmentation.Weka_Segmentation.getProbability");
</source>

== Library use ==

The plugin GUI is independent from the plugin methods. The methods are implemented in a separate file in a library-style fashion, so they can be called from any other Fiji plugin without having to interact with the GUI. This facilitates its integration with other plugins and allows easy scripting.

== License ==
This program is '''free software'''; you can redistribute it and/or modify it under the terms of the '''GNU General Public License''' as published by the Free Software Foundation ([http://www.gnu.org/licenses/gpl.txt http://www.gnu.org/licenses/gpl.txt]).

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details. 

[[Category:Plugins]]
[[Category:Segmentation]]
[[Category:Machine Learning]]
