= Introduction =

In scientific image processing and image analysis, an image is something different than a regular digital photograph of a beautiful scene you shot during your latest vacation.

In the context of science, digital images are samples of information, sampled at vertex points of ''n''-dimensional grids.

== What are pixel values? ==

Human visual perception is very good at certain tasks, such as contrast correction, detecting subtle differences in bright colors, but notoriously bad with other things, such as discerning dark colors, or classifying colors without appropriate reference.  It is therefore very important to keep in mind that the pixel values are numbers, not colors.

For example, when you recorded an image using a confocal microscope, the values you get at a certain coordinate are not color values, but relate to photon counts.

== Pixels are not little squares ==

And voxels are not cubes! See the [http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf whitepaper by Alvy Ray Smith].

== Why (lossy) JPEGs should not be used in imaging ==
JPEG stands for Joint Photographic Experts Group who were the creators of a commonly used method of lossy compression for photographic images. This format is commonly used in web pages and supported by the vast majority of digital photographic cameras and image scanners because it can store images in relatively small files at the expense of image quality. There are also loss-less JPEG modes, but these in general are not widely implemented and chances are that most of the images are of the lossy type.

"'''Lossy compression'''" means that in the process of file size reduction certain amount of image information is discarded. In the case of JPEGs, this might not be readily obvious to an observer, but it will be important for image processing purposes.

"'''Loss-less or non-lossy compression'''" means that the file size is reduced, but the data stored is exactly the same as in the original.

In JPEG lossy compression, therefore, the stored image is not the same as the original, and hence '''not suitable''' for doing serious imaging work. While JPEG compression throws away information that the eye cannot detect easily, this results in considerable image artifacts (variable "blockiness" of the image in groups of 8x8 pixels). Any attempts to process or quantify those images will be affected in uncontrollable ways by the presence of the artifacts. This is particularly obvious in the hue channel of JPEG compressed colour images when converted to HSB colour space.

Example: a section of the famous  [http://sampl.ece.ohio-state.edu/data/stills/color/mandrill.png Mandrill] image. From left to right, you see the original (with an 8-bit colormap), the hue channel of the original, and the hue channel after saving as a JPEG with ImageJ's default options -- note in particular the vertical and horizontal artifacts:

[[Image:Mandrill-orig.png|Original (8-bit colormap)]][[Image:Mandrill-hue-png.png|Hue of the original image]][[Image:Mandrill-hue-jpg.png|Hue of the JPEG image (default ImageJ settings)]]

While most digital cameras save in JPEG format by default, it is very likely that they also support some non-lossy format (such as TIFF or a custom RAW format). Use those formats instead.
A format called JPEG2000 supported by various slide scanners and used in "virtual slide" products was created to improve image quality and compression rates, however both lossy and non-lossy versions the JPEG2000 format exist. Use only the non-lossy formats.

'''Multichannel images''' in particular are harmed by the JPEG format: since the multiple channels are misinterpreted as red, green and blue (while most channels integrate more than just one wavelength), JPEG will shift the colors imperceptibly to improve compression. In the worst case, this can lead to [[Colocalization_Analysis|colocalization]] where there was none.

'''TIFF''' or '''PNG''' formats available in ImageJ/Fiji are non-lossy formats. TIFF preserves any calibrations applied to your images, but images are not compressed. PNG files are smaller as non-lossy compression is applied but they do not store calibration data.

Once an image has been saved as compressed JPEG there is no way of reverting to the original, therefore an old JPEG-compressed image saved again as TIFF or PNG still contains all the original JPEG compression artifacts.



= Considerations during Image Segmentation (Binarization) =

== What is binarization and what is it good for? ==

As the word already suggests, with a binarization you divide your image in two parts usually referred to as foreground and background. This is the simplest method of image segmentation. Simple does not refer to easily achieving good results it is just often perceived by the user as being easier. There are also methods which group pixels of an image into different classes. Those ones are mostly based on statistical approaches. Some are implemented in Fiji such as the "[[Statistical_Region_Merging|Statistical Region Merging]]" plugin or the "[[Linear_Kuwahara|Linear Kuwahara filter]]". For true color images there exist even more sophisticated classification methods often based on machine learning and training by the user with a set of exemplary test images. While image binarization can also be achieved with methods including statistics the simplest would be to just set one or two (upper and lower) cut-off value(s) separating specific pixel intensities from each other. This is often referred to as a threshold (value).
Separating image features and objects by there intensity is often suitable if the intensity is the parameter which directly relates to the spatial characteristics of the objects and defines those. This is for example often the case in fluorescence microscopy with clearly stained cellular structures and low background staining. If other parameters than the intensity define the structures outline or area a simple threshold does often not lead to satisfying results or even fails completely doing the job.
Many users are aware of a method which allows them to manually define such a threshold value but this comes with several limitations...

== Why not simply choose a manual threshold? ==

"I usually define a manual threshold to extract my objects..., is this ok?"
As a recommendation... Whenever possible, try to avoid manual thresholding!!!
Because manual methods have several limitations:

a) no/low reproducibility 

b) high user bias

c) tedious and time-consuming fiddling-around finding an "appropriate" cut-off value

d) incompatibility with automatic processing

e) high intra- and inter-user variability

You might think of determining a fixed manual threshold and then applying the determined cut-off values to all your images. This would at least fulfill the premise to treat all the images which you need to compare equally but will not make you happy in the end. Since there is a certain variability in the images of one single experiment and mostly an even higher variability in between the images of replicate experiments, one fixed value will not achieve to extract similar features from different images. 

So what is the alternative?

Since many decades scientist try to develop automatic segmentation algorithms. Those algorithms might be based on different properties of the images (see below) but they all have in common that the determination of the threshold is based on image-intrinsic properties and not on user decisions. Obviously, if the user has a choice of algorithms the final decision on which algorithm to apply under specific conditions or a specific experiment is left to the user. Nevertheless, this already reduces the bias from e.g. 256 possible cut-off values down to the number of available algorithms. We furthermore need to distinguish between global and local thresholds. During global thresholding the image as whole is taken to determine the cut-off value and this value is applied to each pixel in the image. Local thresholds, as the name suggests, are determined in a local environment (often defined by the radius of a circular neighborhood). Thereafter, this local cut-off values are also locally applied to the individual pixels of the image. Thus, darker areas in an image might be extractable comparably well as very bright areas. This would not be possible with a global threshold, where specifically very dark areas shift into the background or might be under-extracted.

Thus, the advantage of the automatic binarization methods are:

a) they are fully reproducible (on the same image they will also lead to the same binarization result)

b) no bias during thresholding only during decision for a specific automatic algorithm

c) the automatic cut-off value determination accounts for differences e.g. in the image histogram and "react" to this which mostly leads to a preferable feature extraction in comparison to fixed values.

d) easy applicable to image stacks with differentiation between individual image histograms or the complete stack histogram.

e) it's rapid (no fiddling) and perfectly serves automation e.g. in macros

In ImageJ and Fiji, there are so far 16 [[Auto_Threshold|Global Auto Thresholds]] and 9 [[Auto_Local_Threshold|Auto Local Thresholds]] implemented (by Gabriel Landini). Those do an excellent job for many intensity based images.

== Which automatic methods do exist for binarization? ==
Generally, there are different groups of algorithms for image binarization.
(The following classification of methods is taken from Sezgin and Sankur, Survey over image thresholding techniques and quantitative performance evaluation, Journal of Electronic Imaging 13(1), 146â€“165 (January 2004).)

1. histogram shape-based methods, where, for example, the peaks, valleys and curvatures of the smoothed histogram are analyzed.

2. clustering-based methods, where the gray-level samples are clustered in two parts as background and foreground (object), or alternately are modelled as a mixture of two Gaussians.

3. entropy-based methods result in algorithms that use the entropy of the foreground and background regions, the cross-entropy between the original and binarized image, etc.

4. object attribute-based methods search a measure of similarity between the gray-level and the binarized images, such as fuzzy shape similarity, edge coincidence, etc.

5. the spatial methods use higher-order probability distribution and/or correlation between pixels 

6. local methods adapt the threshold value on each pixel to the local image characteristics.

This list might not be comprehensive but gives a good idea about the extensive possibilities available.

== What else is critical during binarization and further object analysis? ==

a) Image acquisition: it is indispensable that during imaging the field of view shows an equal lighting and this is checked and adjusted if necessary. Besides other disadvantages (e.g. no reliable intensity measurements) unequal lighting massively influences the outcome of thresholding methods (especially the global ones). Furthermore, a higher signal-to-noise ratio in e.g. fluorescence based images will positively influence extraction by thresholds. 

b) Pre-processing: since the basis of the determination of any cut-off value during thresholding are the pixel values, any change in those will influence the final binarization result. Thus, the user needs to apply methods which adjust brightness and contrast or any image filters wisely, so that they do not degrade the actual features of interest to extract but rather makes them more pronounced fostering the binarization process.

c) Post-Processing: after obtaining a binary image resulting from any threshold, additional binary operations such as erosion, dilation, opening, closing (all under >Process >Binar) image filters (under >Process >Filters), image combinations by boolean operations (e.g. >Process >Image Calculator),... can be performed. Here the user needs to pay attention to not alter the extraction massively and thus reducing reproducibility and quality of the segmentation. However, binary operations also might be necessary to correct further measurements of area or object counts. Internal holes for example need to be closed (>Process >Binary >Fill Holes) to extract the correct area of particles (this can also be achieved directly during the measurement when using >Analyze >Analyze Particles.... Watershed (or related) separation techniques are necessary when close particles fuse to form clumps/aggregates. 

d) it is for example also recommendable to use such extracted binary objects to create selections from them (e.g. by >Edit >Selection >Create Selection) and use those on original images for intensity based measurements instead of making manual ROIs for measurements. This also comes with the advantage of being very usefull during automation.

e)...

== Is there possibility to check the quality of a binarization ==
Several publications deal with methods to check the quality of image segmentation. Many methods refer to a comparison to a so called "ground truth" in the first place. This is basically a user-created binarization of an example image to test the extraction "quality". But this has to be seen critical since the result is compared to a biased "ground truth". Independent comparisons to different image intrinsic measures (e.g. object edges, entropy, etc.) would be preferable but are rather rare. The "[[BioVoxxel_Toolbox#Threshold_Check|Threshold Check]]", [http://www.cscjournals.org/manuscript/Journals/IJIP/volume8/Issue2/IJIP-829.pdf] e.g is also just a trial to make it easier for the untrained user, to decide for one or the other threshold. The semi-quantitative evaluation therein is also biased but might facilitate a rather unbiased comparison of different extraction methods (based on a user-chosen and thus biased reference value).

== What should I do with true color images (e.g. histological staining) with different color and not a single intensity scale? ==

Here again, many different possibilities exist:

1.) One would be related to the manual thresholding methods mentioned above as not ideal. But this might be combined partially with automatic intensity thresholding and should be mentioned here out of completeness and since it might lead to good results e.g. for some histological staining. The "Color Threshold" also implemented by Gabriel Landini gives access to different color space representations of a true color image. This means that the user can select specific ranges of color tones (hue) and color saturation as well as their brightness (e.g. using the HSB color space). In this case, a limitation to specific color tones using the hue value slider is possible with a rather low bias (if I want only blue, I exclude all the other colors, which is easier to determine than an intensity cut-off value). Saturation is already more tricky to set with low-subjectiveness. For the thresholding of the brightness the standard Auto Thresholds can be applied, thus reducing the bias slightly. 

2.) Similar to the method described above, the image can be split into the respective channels of different color spaces (HSB, CIELAB,...) and those channels can then be automatically thresholded since all of them are based on a single intensity scale. It might already be sufficient to extract the desired information by thresholding only one of the color space channels. A combination of the extraction results from two or all three channels is also possible by using boolean functions (such as AND, OR, XOR) where applicable.

3.) [[Colour_Deconvolution|Color deconvolution]] might also be a solution. But here the vector definition with individual staining components is an important step which is nicely implemented in the tool and should be done for the sake of consistenca and accuracy. Using the definition by ROIs again introduces low reproducibility due to biased, user defined areas of "a color". This is a fast option but the problem here is, that if this is done on the image which should be segmented the colors in a brightfield image exist usually in a mixed fashion meaning that the user defines already a mixture of colors to later on separate exactly those. This most likely leads to inconsistent results even though it might visually look good in the first place.

4.) Usage of specific tools based on machine learning might be helpful. Here, the user is required to do some training on representative example images. This is achieved by selecting areas which should be assigned to the foreground or background, respectively. This is obviously also biased, with a good training (potentially by different experts) the feature extraction contains a lower bias, since the same trained classifier is applied to the different images. As ImageJ plugins available are here the [[SIOX:_Simple_Interactive_Object_Extraction|SIOX: Simple Interactive Object Extraction]] and the [[Trainable_Weka_Segmentation|Trainable WEKA Segmentation]].

5.) ...
